# Define here the models for your spider middleware
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/spider-middleware.html

from scrapy import signals

# useful for handling different item types with a single interface
from itemadapter import is_item, ItemAdapter


class AppRankSpiderSpiderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the spider middleware does not modify the
    # passed objects.

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_spider_input(self, response, spider):
        # Called for each response that goes through the spider
        # middleware and into the spider.

        # Should return None or raise an exception.
        return None

    def process_spider_output(self, response, result, spider):
        # Called with the results returned from the Spider, after
        # it has processed the response.

        # Must return an iterable of Request, or item objects.
        for i in result:
            yield i

    def process_spider_exception(self, response, exception, spider):
        # Called when a spider or process_spider_input() method
        # (from other spider middleware) raises an exception.

        # Should return either None or an iterable of Request or item objects.
        pass

    def process_start_requests(self, start_requests, spider):
        # Called with the start requests of the spider, and works
        # similarly to the process_spider_output() method, except
        # that it doesn’t have a response associated.

        # Must return only requests (not items).
        for r in start_requests:
            yield r

    def spider_opened(self, spider):
        spider.logger.info('Spider opened: %s' % spider.name)


class AppRankSpiderDownloaderMiddleware:
    # Not all methods need to be defined. If a method is not defined,
    # scrapy acts as if the downloader middleware does not modify the
    # passed objects.

    @classmethod
    def from_crawler(cls, crawler):
        # This method is used by Scrapy to create your spiders.
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    def process_request(self, request, spider):
        # Called for each request that goes through the downloader
        # middleware.

        # Must either:
        # - return None: continue processing this request
        # - or return a Response object
        # - or return a Request object
        # - or raise IgnoreRequest: process_exception() methods of
        #   installed downloader middleware will be called
        return None

    def process_response(self, request, response, spider):
        # Called with the response returned from the downloader.

        # Must either;
        # - return a Response object
        # - return a Request object
        # - or raise IgnoreRequest
        return response

    def process_exception(self, request, exception, spider):
        # Called when a download handler or a process_request()
        # (from other downloader middleware) raises an exception.

        # Must either:
        # - return None: continue processing this exception
        # - return a Response object: stops process_exception() chain
        # - return a Request object: stops process_exception() chain
        pass

    def spider_opened(self, spider):
        spider.logger.info('Spider opened: %s' % spider.name)

        
import random
        
class UserAgentMiddleware(object):
    def __init__(self):
        self.user_agent_list=[
            "Mozilla/5.0 (Linux; Android 5.0; SM-N9100 Build/LRX21V) > AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 > Chrome/37.0.0.0 Mobile Safari/537.36 > MicroMessenger/6.0.2.56_r958800.520 NetType/WIFI",
            "Mozilla/5.0 (iPhone; CPU iPhone OS 7_1_2 like Mac OS X) > AppleWebKit/537.51.2 (KHTML, like Gecko) Mobile/11D257 > MicroMessenger/6.0.1 NetType/WIFI"
        ]
    def process_request(self, request, spider):
        user_agent=random.choice(self.user_agent_list)
        request.headers['User-Agent']= user_agent

        
        
        
class wandoujiaNormalProxy(object):
    def __init__(self):
        self.ip_proxy = ip_proxy()
    
    def process_request(self, request, spider):
        proxy_ip = self.ip_proxy.get_next_ip()
        request.meta["proxy"] = proxy_ip
        print(f"appProxyMiddleware --> {proxy_ip}")

        
        
    
    
    
# 获取代理    
import requests

class ip_proxy:
    def __init__(self):
        self.ip_list = []
        self.query_index = 0
   
    
    def get_ip_list(self):
        self.ip_list = []
        self.query_index = 0
        api_url = "https://proxyapi.horocn.com/api/v2/proxies?order_id=1XOP1702794312076199&num=10&format=text&line_separator=unix&can_repeat=yes&user_token=6bb1b77751f48b087ffb2ef64c9d88eb"
        headers = {
            'User-Agent': 'Mozilla/4.0 (compatible; MSIE 9.0; Windows NT 6.1)'
        }
    
        data = requests.get(api_url,headers=headers).text.split('\n')
        for url in data:
        
            http_proxy = "http://"+url
#             https_proxy = "https://"+url

            self.ip_list.append(http_proxy)
#             self.ip_list.append(https_proxy)

            
        print("IP列表更新成功，共获取{ip_num}个IP，".format(ip_num= len(data) ))

    
    # 获取下一个IP
    def get_next_ip(self):
        # 如果消耗完毕，更新一下
        if (self.query_index + 1) > len(self.ip_list):
            print("IP列表消耗完毕，即将更新")
            self.get_ip_list()
            
        back_data = self.ip_list[self.query_index]
        self.query_index+=1
        print("获取第{index}个ip：{ip}".format(index=self.query_index,ip=back_data) )
        return back_data
                 