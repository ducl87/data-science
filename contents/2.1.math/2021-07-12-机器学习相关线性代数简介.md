---
layout: post
title: '机器学习相关线性代数简介（持续完善中）'
date: 2021-07-12
tags:
  线性代数
  linear-algebra
  data-science
  vector
  matrix
---
> 参考资料：
>
> * [mathematics for machine learning](https://mml-book.github.io/)
> * [Understanding Vectors From a Machine Learning Perspective](https://neptune.ai/blog/understanding-vectors-from-a-machine-learning-perspective)
> * [Introduction to Linear Algebra for Applied Machine Learning with Python](https://pabloinsente.github.io/intro-linear-algebra)
> * [Matrices: Gaussian & Gauss-Jordan Elimination](https://www.craftonhills.edu/current-students/tutoring-center/mathematics-tutoring/matrices-gauss-jordan.pdf)
> * [Transformations and Matrices](https://www.mathsisfun.com/algebra/matrix-transform.html)
> * [The Geometry of the Dot and Cross Products](https://www.maa.org/sites/default/files/images/images/upload_library/4/vol6/Dray2/Dray.pdf)
> * [Projections onto subspaces](https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/least-squares-determinants-and-eigenvalues/projections-onto-subspaces/MIT18_06SCF11_Ses2.2sum.pdf)
> * [MATH 240: Vector Spaces](http://www.math.niu.edu/~beachy/courses/240/06spring/vectorspace.html)
> * [维基百科-线性子空间](https://zh.wikipedia.org/wiki/%E7%BA%BF%E6%80%A7%E5%AD%90%E7%A9%BA%E9%97%B4)
> * [Two additional vector spaces associated with a matrix](https://people.math.carleton.ca/~kcheung/math/notes/MATH1107/wk09/09_column_space_row_space.html)
> * [Basis, Dimension, Rank](http://sites.oxy.edu/vgrigoryan/214/lecs/11.Basis_Dim_Rank.pdf)
> * [2.7Basis and Dimension](https://textbooks.math.gatech.edu/ila/dimension.html)
> * 起始编辑时间：2021-07-12

[toc]

## 目录截图：



## 一、向量（vectors）

在数学和物理中，向量是向量空间（一种集合，具体见下文）的一个元素，我们常说的向量一般指几何向量，但是在现代数学中，“向量”的概念不限于此，满足下列公理的任何数学对象都可以当做向量处理。

**公理化定义：**

给定域$F$，$F$上的向量空间$V$是一个集合，其上定义了两种运算：

* **向量加法：**$V+V\rarr V$​​​​，把向量空间$V$​​​​中的两个元素$u,v$​映射到$V$​​​​中另一个元素，记作$u+v$；
* **向量乘法：**$F \times V \rarr V$​，把$F$中的一个元素$a$和$V$​中一个元素$u$​变为$V$中的另一个元素，记作$a\cdot u$

$V$称为向量空间，其​中的元素称为向量，相对地，$F$​中的元素称为标量。

### 1.1 向量在机器学习中的地位

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210909113719.png?x-oss-process=style/wp)

如上图，典型的机器学习模型中常用向量（也有矩阵）来进行运算，分别在以下阶段：

* 输入

机器无法像人类一样直接阅读文字或理解图片，我们需要将这些编码为数字（向量和矩阵形式），比如在做房价预测模型时，房子的面积、房间数、层数等特征信息可以就可以统一编码为向量。

* 模型

机器学习的模型本质上是一个函数，需要将输入的特征向量信息运算得到新的向量

* 输出

可以用向量来表达回归的数值、分类。

### 1.1类型

#### 1.1.1 几何向量：

我们高中时学到、见到的最多的类型，一般在二维或三维中作图就可以表示

<img src="https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210712154844.png?x-oss-process=style/wp" style="zoom:50%;" />



#### 1.1.2 多项式：

多项式如$f(x)=x^2+y+1$，之所以也称为向量是因为它满足向量的定义：

* 可以用加法，得到新的多项式；
* 可以用乘法，得到新的多项式；

如$f(x)+g(x)$和$5\times f(x)$

<img src="https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210712155230.png?x-oss-process=style/wp" style="zoom:50%;" />



#### 1.1.3 实数集合：

广义地说，任意实数组成的集合 ${\Bbb R}^n$ 也是向量，这是机器学习中最常见、最重要的向量：



$$
X = 
\begin{bmatrix}
x_1 \\
x_2 \\
. \\
. \\
. \\
x_n
\end{bmatrix}
\in 
{\Bbb R}^n
$$


如，一个$  {\Bbb R}^3$ 3维的向量：


$$
x = 
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
\end{bmatrix}
\in
{\Bbb R}^3
$$



### 1.2向量的基本运算

#### 1.2.1 加法

两个大小一样的向量的加法，需要向量的每个元素逐个相加（element-wise）：


$$
x+y =
\begin{bmatrix}
x_1 \\
. \\
. \\
. \\
x_n
\end{bmatrix}
+
\begin{bmatrix}
y_1 \\
. \\
. \\
. \\
y_n
\end{bmatrix}
=
\begin{bmatrix}
x_1 + y_1 \\
. \\
. \\
. \\
x_n + y_n
\end{bmatrix}
$$


如


$$
x+y = 
\begin{bmatrix}
1 \\
2 \\
3 \\
\end{bmatrix}
+
\begin{bmatrix}
3 \\
4 \\
5 \\
\end{bmatrix}
=
\begin{bmatrix}
1 + 3 \\
2 + 4 \\
3 + 5
\end{bmatrix}
=
\begin{bmatrix}
4 \\
6 \\
8 
\end{bmatrix}
$$


向量加法满足以下性质：

* 交换律（Commutativity）：$x + y = y + x$
* 结合律（Associativity）：$(x+y)+z=x+(y+z)$
* 零向量$\vec{0}$无效：$x+0 = x$
* 减去自身等于零向量$\vec{0}$：$x-x = \vec{0}$

在`numpy`中，使用运算符`+`或`add`方法计算两个向量的和

```python
import numpy as np
# 赋值x,y为向量[1,2,3]
x = y = np.array([[1],
                  [2],
                  [3]])
>>> x + y
array([[2],
       [4],
       [6]])

>>> np.add(x,y)
array([[2],
       [4],
       [6]])
```

#### 1.2.2向量与标量相乘（vector-scalar multiplication）

向量与标量的乘法也遵循逐个元素操作的原则（element-wise）：


$$
ax = 
\begin{bmatrix}
ax_1 \\
. \\
. \\
. \\
ax_n \\
\end{bmatrix}
$$



假设$a=3$，$x=[1,2,3]^T$，则


$$
ax = 3
\begin{bmatrix}
1 \\
2 \\
3 \\
\end{bmatrix}
=
\begin{bmatrix}
3\times1 \\
3\times2 \\
3\times3 \\
\end{bmatrix}
=
\begin{bmatrix}
3 \\
6 \\
9 \\
\end{bmatrix}
$$


向量与标量的乘法满足以下性质：

* 结合律（Associativity）：$(\alpha\beta)x=\alpha(\beta x)$
* 左分配率（Left-distributive）：$(\alpha+\beta)x=\alpha x+ \beta x$
* 右分配率（Right-distributive）：$x(\alpha+\beta)=x\alpha + x\beta $
* 其他：$\alpha(x+y)=\alpha x + \alpha y$

在`numpy`中，使用运算符`*`或`np.multiply`计算向量与标量乘积

```python
import numpy as np

x = 3
y = np.array([
    [3],
    [4],
    [5],
    [6]
])

>>> x*y
array([[ 9],
       [12],
       [15],
       [18]])
```

#### 1.2.3向量的线性组合

$$
\alpha x  + \beta y = \alpha 
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
+
\beta
\begin{bmatrix}
y_1 \\
y_2
\end{bmatrix}
=
\begin{bmatrix}
\alpha x_1+ \beta y_1 \\
\alpha x_2 + \beta y_2
\end{bmatrix}
$$



如 $\alpha =2 , \beta=3,x=[2,3]^T,y=[4,5]^T$，则


$$
\alpha x + \beta y = 
2 
\begin{bmatrix}
2 \\
3
\end{bmatrix}
+
3
\begin{bmatrix}
4 \\
5
\end{bmatrix}
=
\begin{bmatrix}
2 \times 2 + 3 \times 4 \\
2 \times 3 + 3 \times 5
\end{bmatrix}
=
\begin{bmatrix}
16 \\
21
\end{bmatrix}
$$


线性组合的另一种方式是用求和公式，假设$x_i,...x_k$是一个向量，标量集合$\beta_i,...,\beta_k\in {\Bbb R}$，则


$$
\sum_{i=1}^k \beta_i x_i := \beta_1x_1 + ...+\beta_kx_k
$$


公式中$:=$表示“定义为”的意思。

在`numpy`中，计算向量的线性组合

```python
import numpy as np
a,b = 2,3
x,y = np.array([[2],[3]]), np.array([[4],[5]])

>>> a * x + b * y
array([[16],
       [21]])
```



#### 1.2.4 向量与向量相乘：点乘（dot product）

点乘也称为向量的内积（inner product），两个向量$x_1,x_2$​的点乘定义如下：


$$
x_1 \cdot x_2 := 
\begin{bmatrix}
x_1 \\
y_1
\end{bmatrix}
^T
\begin{bmatrix}
x_2 \\
y_2
\end{bmatrix}
=
\begin{bmatrix}
x_1 \ y_1
\end{bmatrix}
\begin{bmatrix}
x_2 \\
y_2
\end{bmatrix}=  x_1 x_2 + y_1 y_2
$$
其中$T$，表示“转置”，如：


$$
x \cdot y = 
\begin{bmatrix}
-2 \\
2
\end{bmatrix}
\cdot
\begin{bmatrix}
4 \\
-3
\end{bmatrix}
=
\begin{bmatrix}
-2  &2
\end{bmatrix}
\begin{bmatrix}
4 \\
-3
\end{bmatrix}
= -2 \times 4 + 2 \times(-3) = -14
$$


在`numpy`中，使用`.T`获取向量的转置，然后使用`@`或`.dot()`计算矩阵乘法：

```python
import numpy as np
x = np.array([
    [-2],
    [2]
])
y = np.array([
    [4],
    [-3]
])

>>> x.T
array([[-2,  2]])

>>> x.T @ y
array([[-14]])

>>> np.dot(x.T,y)
array([[-14]])

```



### 1.3 向量空间（space）、生成空间（span）、子空间（subspaces）、空白空间（null space）

#### 1.3.1 向量空间

首先回忆一下什么是向量的定义：

**公理化定义：**

给定域$F$，$F$上的向量空间$V$是一个集合，其上定义了两种运算：

* **向量加法：**$V+V\rarr V$​​​​，把向量空间$V$​​​​中的两个元素$u,v$​映射到$V$​​​​中另一个元素，记作$u+v$；
* **向量乘法：**$F \times V \rarr V$​，把$F$中的一个元素$a$和$V$​中一个元素$u$​变为$V$中的另一个元素，记作$a\cdot u$

$V$称为向量空间，其​中的元素称为向量，相对地，$F$​​中的元素称为标量。



根据定义，几何向量、多项式等都可以形成向量空间，如一个${\Bbb R}^{2\times1}$​​​​的向量和${\Bbb R}^{3\times1}$​​​的向量形成的向量空间完全不同（一个是二维，一个是三维）​，数学家在建立线性代数这门分支学科时，约定好了向量空间必须满足的8条公理，以便于后续研究的性质特点适用于所有向量空间：



对于域$F$中任意元素$a,b$以及向量空间$V$中任意元素$u,v,w$都成立：

| 公理             | 说明                                              |
| ---------------- | ------------------------------------------------- |
| 向量加法的结合律 | $u+(v+w)$                                         |
| 向量加法的交换律 | $u+v=v+u$                                         |
| 向量加法的单位元 | 存在一中零向量$0 \in V$​，对于任意$u$，满足$0+u=u$ |
| 逆元素           | 对于任意$v$​​都存在其逆元素$-v$​，满足$v+(-v)=0$     |
| 标量乘法         | $a(b\bold v)=(ab)\bold v$                         |
| 向量乘法的单位元 | $1\bold v=\bold v$                                |
| 分配率           | $a(\bold u+\bold v)=a\bold u+a\bold v$            |
| 分配率           | $(a+b)\bold v=a\bold v+ b \bold v$                |

> 向量空间的概念可能有点难以理解，问向量空间是什么，有点类似于问”3是什么？“
>
> 3可以是3个人，3束花，3个立方体等等，不管他们是什么，他们都满足于3这个数字所对应的数学计算规律，如四则运算。



#### 1.3.2 向量生成空间

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210712212634.png?x-oss-process=style/wp)

一组向量$\bold x,\bold y$和一组标量$\alpha ,\beta$​，线性组合$\alpha \bold x+\beta \bold y$所有的取值称为向量空间$span(x,y)$。

* 如果$\bold x ,\bold y$​方向相同，则$span(x,y)$​还是在这条线上；
* 如果$\bold x ,\bold y$方向不同，则$span(x,y)$可以扩展到整个二维平面；
* 以此类推，三维四维......

> $\bold x ,\bold y$​方向相同，在多重共线性（multicollinearity）中表示共线（colinear），这时候他们表示的信息可能相同，在机器学习特征处理时，可以扔掉一些冗余信息。

#### 1.3.3 向量子空间

顾名思义，向量子空间（也称线性子空间）是向量空间的子集，定理如下：

给定域$F$，$F$上的向量空间$V$是一个集合，并设$W$是$V$的子集。则$W$是向量子空间，当且仅当它满足下列三个条件：

1. 零向量$0$在$W$中；
2. $\forall w_i \in W \rarr w_1+w_2 \in W$​​，即子空间中的向量之和还在子空间内；
3. $\bold u\in W,a \in F \rarr a \bold u \in W$，即子空间中的向量与标量积还在子空间内。



![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210815111021.png?x-oss-process=style/wp)

> 根据子空间的定义，可以想象一个子空间内的向量不管做标量乘法还是加法（与子空间中另一个向量）都无法“跳出”这个子空间。



#### 1.3.4 向量空白空间

待完善



### 1.4 线性相关（dependence）和线性无关（independence）

一组矢量如果是线性相关的，则至少有一个向量可以用组内其他向量的线性组合表示，如下图，左边$\bold x, \bold y, \bold z$是线性相关的。

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210815115058.png?x-oss-process=style/wp)



更准确的定义如下：

假设有一组向量$\bold x_1,\bold x_2, \cdots, \bold x_k$和实数集合$\beta \in {\Bbb R}$，如果标量$\beta_i$不全为0时，存在$\bold 0 = \sum_{i=1}^k \beta_i \bold x_i$，则这组向量是**线性相关**的；

类似的，假设有一组向量$\bold x_1,\bold x_2, \cdots, \bold x_k$​和实数集合$\beta \in {\Bbb R}$​，如果仅当标量$\beta_i$​全为0时，才满足$\bold 0 = \sum_{i=1}^k \beta_i \bold x_i$​​，则这组向量是**线性无关**的；

> 后续小节内容会不断引用线性相关的概念，到时候你会有更深刻的理解。我们暂时可以这样理解，一组线性相关的向量包含冗余的信息，而线性无关则不。



### 1.5 向量范数（vector norms）

在机器学习算法中，经常使用到向量的范数，范数可以简单地理解为向量的长度（从原点到终点），如图，一般常用的范数有以下几种：

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210713165010.png?x-oss-process=style/wp)



#### 1.5.1 欧几里得范数（Euclidean norm）

欧几里得范数$L_2$最常用，它的定义为：


$$
||x||_2 := \sqrt{\sum_{i=1}^n x_i^2}
$$


如，二维向量的$L_2$范数为：


$$
||x||_2 \in {\rm I\!R} ^2 = \sqrt{x_1^2+x_2^2}
$$


在`numpy`中，用`np.linalg.norm`计算范数：

```python
import numpy as np
a = np.array([[3],[4]])
>>> np.linalg.norm(a,2)
5.0
```

上图第一个坐标图表达的是$L_2$范数为1的二维向量形成的区域，可以看到它是一个圆，之所以是一个圆是因为：



假设向量$a = [x,y]$是一个二维向量，根据$L_2$范数定义：


$$
||a||_2=\sqrt{x^2+y^2}
$$


正好$x,y$分别是向量在坐标系中的横坐标和纵坐标，所以$L_2$长度正好是三角形的斜边。



<img src="https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210713172736.png?x-oss-process=style/wp" style="zoom:50%;" />



#### 1.5.2 曼哈顿范数（Manhattan norm）

曼哈顿范数$L_1$，定义如下：


$$
||x||_1 :=\sum_{i=1}^n |x_i|
$$



如，二维向量的$L_1$范数为：


$$
||x||_1 \in {\rm I\!R} ^2 = |x_1|+|x_2|
$$


在`numpy`中，用`np.linalg.norm`计算范数：

```
import numpy as np
a = np.array([[3],[-4]])
>>> np.linalg.norm(a,1)
7.0
```

如下图，是$L_1$范数为1的二维向量形成的区域，可以看到它是一个方形（之所以叫曼哈顿范数，是因为$L_1$类似曼哈顿的网格型地形）。

<img src="https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210713180519.png?x-oss-process=style/wp" style="zoom: 50%;" />

如下图，之所以$L_1$之所以是一个方形是因为：

假设向量$a = [x,y]$是一个二维向量，根据$L_1$范数定义


$$
||a||_1=|x|+|y|
$$


正好$x,y$分别是向量在坐标系中的横坐标和纵坐标，所以$L_1$长度正好是三角形的两个直角边的长度和，又因为$y=z$（等腰直角三角形），所以


$$
L_1=|x|+|y|=|x|+|z|=1
$$


<img src="https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210713181531.png?x-oss-process=style/wp" style="zoom:50%;" />



#### 1.5.3 最大值范数（Max norm）

最大值范数很简单，就是取向量元素绝对值最大的值即可：


$$
||x||_{\infty} :=  max_i|x_i|
$$

例如，假设向量$x = [3,-4,-5]$的
$$
||x||_\infty=5
$$


在`numpy`中，用`np.linalg.norm`计算范数：

```python
import numpy as np
a = np.array([[3],[-4],[-5]])
>>> np.linalg.norm(a,np.inf)
5.0
```

### 1.6 向量内积，长度和距离（inner product,length and distance）

> 向量的内积、长度分别对应上面说的点乘、范数，但本质上他们是不同的概念

两个向量$x,y \in{\Bbb R}^n$​的内积$ \langle x,y  \rangle$​​定义如下：
$$
\langle x,y \rangle := x \cdot y = \sum_{i=1}^n x_i y_i
$$
即对两个向量执行对应位一一相乘再求和，前面介绍点乘时已经介绍了`numpy`如何计算点乘：

```python
x.T @ y
```



#### 1.6.1 长度（length）

向量的长度定义如下：
$$
||x|| :=\sqrt{\langle x,x \rangle}   = \sqrt{x\cdot x}  
$$


可以看到这个和$L_2$​​范数是一致的，所以有时我们常说“用$L_2$范数计算向量的长度”
$$
||x||_2 := \sqrt{\sum_{i=1}^n x_i^2}
$$


如对于二维向量$x=[3,4]^T$​​​，则


$$
||x|| = \sqrt{3^2+4^2} = 5
$$
在`numpy`中，可以用以下三种方法计算向量长度：

```python
x = np.array([
    [3],
    [4]
])

# 点乘方法
>>> np.sqrt( x.T @ x )
array([[5.]])

>>> np.sqrt(np.dot(x.T,x))
array([[5.]])

# 范数方法
>>> np.linalg.norm(x,2)
5.0

```



#### 1.6.2 距离（distance）

两个向量$x_1,x_2$​​之间的距离定义如下：


$$
d(x_1,x_2) := ||x_1-x_2|| 
$$
即两个向量之间的距离等于向量差的长度，如两个向量$x_1 = [1,3]^T, x_2 = [-2,4]^T$​​​，用`numpy`计算他们之间的距离为：

```python
x = np.array([
    [1],
    [3]
])
y = np.array([
    [-2],
    [4]
])

>>> np.sqrt( (x-y).T @ (x-y) )
array([[3.16227766]])

>>> np.linalg.norm(x-y,2)
3.1622776601683795

```



#### 1.6.3 向量角度和正交性（orthogonality）

在机器学习领域，向量的角度用于测量向量之间的相似度（特征的相似度）。

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210802165036.png?x-oss-process=style/wp)



两个向量$x,y$之间的夹角设为$\theta$​，经过证明（证明过程参见我另一篇文章[向量内积的几何意义](https://enpeizhao.github.io/2021/07/23/%E5%90%91%E9%87%8F%E5%86%85%E7%A7%AF%E7%9A%84%E5%87%A0%E4%BD%95%E6%84%8F%E4%B9%89%E8%AF%81%E6%98%8E%E8%BF%87%E7%A8%8B/)）可以得到：


$$
cos\theta = \frac{\langle x,y\rangle}{||x||||y||}
$$
因为$cos\theta \in [-1,1]$​​，所以有：


$$
-1 \leq	 \frac{\langle x,y\rangle}{||x||||y||} \leq	 1
$$
我们可以得到**Cauchy-Schwarz不等式**


$$
| \langle x,y \rangle | \leq  ||x||||y||
$$
如果两个向量$x,y$​内积为0，即$\langle x,y \rangle=0$​，我们称$x,y$​正交（orthogonal），记为$x\perp y$​​：

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210802171902.png?x-oss-process=style/wp)



我们可以在`numpy`中计算夹角 $cos\theta$

```python
x = np.array([[2], [0]])
y = np.array([[0], [2]])

cos_theta =( x.T @ y) / (np.linalg.norm(x,2) * np.linalg.norm(y,2) )

>>> cos_theta
array([[0.]])

```



### 1.7 线性方程组（systems  of linear equations）

一般线性方程组如下：


$$
\begin{align*}
&x_1 &+ x_2 &+x_3 = 3 \\
&x_1 &- x_2 &+ 2x_3 = 2 \\
&2x_1 & &+ 3x_3=5 
\end{align*}
$$


将方程组系数$a_{ij}$挑出，组成向量（vectors）形式：


$$
\begin{bmatrix}
a_{11}\\
.\\
.\\
.\\
a_{m1}
\end{bmatrix}
x_1 
+ 
\begin{bmatrix}
a_{12}\\
.\\
.\\
.\\
a_{m2}
\end{bmatrix}
x_2

+ ... + 
\begin{bmatrix}
a_{1n}\\
.\\
.\\
.\\
a_{mn}
\end{bmatrix}
x_n
=
\begin{bmatrix}
b_1\\
.\\
.\\
.\\
b_m
\end{bmatrix}
$$



再将向量变成矩阵（matrices）形式：


$$
\begin{bmatrix}
&a_{11} &...&a_{1n}\\
&. & &. \\
&. & &. \\
&. & &. \\
&a_{m1} &...&a_{mn}
\end{bmatrix}
\begin{bmatrix}
x_1 \\
. \\
. \\
. \\
x_n
\end{bmatrix}
=
\begin{bmatrix}
b_1 \\
. \\
. \\
. \\
b_n
\end{bmatrix}
$$



## 二、矩阵（matrices）

一个有着$m$行$n$列的矩阵$A\in {\Bbb R}^{m \times n}$的定义如下：


$$
A :=
\begin{bmatrix}
&a_{11} & a_{12} &... &a_{1n} \\
&a_{21} & a_{12} &... &a_{2n} \\
&⋮ &⋮  &⋱ &⋮ \\
&a_{m1} & a_{m2} &... &a_{mn} \\

\end{bmatrix}
,
a_{ij} \in {\rm I\!R}
$$


在`numpy`中，构造一个矩阵：

```python
import numpy as np
A = np.array([
    [1,7],
    [3,5]
])

>>> A
array([[1, 7],
       [3, 5]])
```



### 2.1 基本运算

#### 2.1.1 加法

矩阵加法遵循逐个元素操作的原则（element-wise），矩阵$A\in{\Bbb R}^{m\times n}$和$B\in {\Bbb R}^{m\times n}$的和为：


$$
A+B :=
\begin{bmatrix}
&a_{11}+b_{11} & a_{12} + b_{12} &... &a_{1n} + b_{1n}\\
&a_{21}+b_{21} & a_{22} + b_{22} &... &a_{2n} + b_{2n}\\
&⋮ &⋮  &⋱ &⋮ \\
&a_{m1} +b_{m1} & a_{m2}+ b_{m2} &... &a_{mn} +b_{mn}\\

\end{bmatrix}
\in
{\rm I\!R}^{m\times n}
$$



例如：


$$
A=
\begin{bmatrix}
0 &2 \\
1 &4
\end{bmatrix},
B=
\begin{bmatrix}
3 &1 \\
-3 &2
\end{bmatrix}
,
则 \ A+B=
\begin{bmatrix}
3+0 &2+1 \\
1-3 &4+2
\end{bmatrix}
=
\begin{bmatrix}
3 &3 \\
-2 &6
\end{bmatrix}
$$



在`numpy`中，类似向量的操作，用`+`或`np.add`运算：

```python
A = np.array([
    [0,2],
    [1,4]
])
B = np.array([
    [3,1],
    [-3,2]
])

>>> A+B
array([[ 3,  3],
       [-2,  6]])
>>> np.add(A,B)
array([[ 3,  3],
       [-2,  6]])
```



#### 2.2.2矩阵与标量相乘（Matrix-scalar multiplication）

矩阵与标量的乘法也遵循逐逐个元素操作的原则（element-wise）：矩阵$A$与标量$a$相乘，使矩阵每个元素$a_{ij}$都乘以$a$，即$a_{ij}\times a$

如：


$$
2 \times 
\begin{bmatrix}
0 &2 \\
1 &4
\end{bmatrix} 
=
\begin{bmatrix}
2\times 0 &2\times 2 \\
2\times 1 &2\times 4
\end{bmatrix} 
=
\begin{bmatrix}
0 &4 \\
2 &8
\end{bmatrix}
$$



在`numpy`中，使用运算符`*`或`np.multiply`计算：

```python
A = np.array([
    [0,2],
    [1,4]
])

>>> 2 * A
array([[0, 4],
       [2, 8]])
>>> np.multiply(2,A)
array([[0, 4],
       [2, 8]])
```



#### 2.2.3矩阵与矩阵相乘（Matrix-matrix multiplication）

矩阵$A \in {\Bbb R}^{m\times n}$与$B \in {\Bbb R}^{n \times p}$相乘，得到新的矩阵$C\in {\Bbb R}^{m\times p}$，新矩阵的


$$
\begin{align*}
A \cdot B &:=
\begin{bmatrix}
&a_{11} & a_{12} &... &a_{1n} \\
&a_{21} & a_{12} &... &a_{2n} \\
&⋮ &⋮  &⋱ &⋮ \\
&a_{m1} & a_{m2} &... &a_{mn} \\

\end{bmatrix}
\begin{bmatrix}
&b_{11} & b_{12} &... &b_{1p} \\
&b_{21} & b_{12} &... &b_{2p} \\
&⋮ &⋮  &⋱ &⋮ \\
&b_{n1} & b_{n2} &... &b_{np} \\

\end{bmatrix}
\\
&=
\begin{bmatrix}
&c_{11} & c_{12} &... &c_{1p} \\
&c_{21} & c_{12} &... &c_{2p} \\
&⋮ &⋮  &⋱ &⋮ \\
&c_{m1} & c_{m2} &... &c_{mp} \\

\end{bmatrix}

\end{align*}
$$



其中$C$具体元素的值定义如下：


$$
c_{ij} :=\sum_{k=1}^n a_{ik}b_{kj},i=1,\dots m, \ j=1,\dots p
$$



如：


$$
A \cdot B = 
\begin{bmatrix}
0 &2  \\
1 &4  \\
3 &1 
\end{bmatrix} 
\begin{bmatrix}
0 &2 \\
1 &4
\end{bmatrix} 
=
\begin{bmatrix}
0 \times 0+2\times 1  &0\times 2 + 2 \times 4 \\
1 \times 0+4\times 1  &1\times 2 + 4 \times 4 \\
3 \times 0+1\times 1  &3\times 2 + 1 \times 4 \\

\end{bmatrix} 
=
\begin{bmatrix}
2 &8 \\
4 &18 \\
1 &10 
\end{bmatrix}
$$



矩阵乘法满足以下性质：

* 结合律（Associativity）：$(AB)C=A(BC)$
* 带标量乘法的结合律：$a(AB)=(aA)B$
* 分配率：$A(B+C)=AB+AC$ 
* 转置乘法：$(AB)^T = B^TA^T$

**需要注意**：$AB\ne BA$

在`numpy`中，使用`@`或`dot`计算：

```python
A = np.array([
    [0,2],
    [1,4],
    [3,1],    
])
B = np.array([
    [0,2],
    [1,4]
])

>>> A @ B
array([[ 2,  8],
       [ 4, 18],
       [ 1, 10]])

>>> np.dot(A,B)
array([[ 2,  8],
       [ 4, 18],
       [ 1, 10]])
```



#### 2.2.4 单位矩阵（identity matrix）

对角线位置为1，其他位置全部为0的正方形矩称为单位矩阵，用$I_n\in {\Bbb R}^{n\times n}$表示：


$$
I_n:=
\begin{bmatrix}
1 & 0 & ⋯ & 0 & 0 \\
0 & 1 & ⋯ & 0 & 0 \\
0 & 0 & ⋱ & 0 & 0 \\
0 & 1 & ⋯ & 1 & 0 \\
0 & 1 & ⋯ & 0 & 1 \\
\end{bmatrix} 
\in
{\Bbb R}^{n\times n}
$$


如：


$$
I_3 =
\begin{bmatrix}
1 &0 &0  \\
0 &1 &0 \\
0 &0 &1
\end{bmatrix}
$$


根据矩阵乘法规则，可以得出$A^{m\times n}\cdot I_n = A^{m \times n}$，即矩阵与单位矩阵相乘等于自身，$I_n$这类似与实数乘法中的$1$。

在`numpy`中，使用`identity`生成单位矩阵：

```python
A = np.array([
    [0,2],
    [1,4],
    [3,1],    
])

I = np.identity(2)

>>> A @ I
array([[0., 2.],
       [1., 4.],
       [3., 1.]])
```

#### 2.2.5 逆矩阵（matrix inverse）

我们知道，实数运算中，$x \times x^{-1} = 1$，类似的，在矩阵中我们定义$A\in {\Bbb R}^{m \times n}$的逆矩阵为$A^{-1}$，它有如下性质：


$$
A^{-1} A=I_n=AA^{-1}
$$


之所以关系逆矩阵，是因为它可以用来解线性方程组，假设一个方程为：


$$
Ax=y
$$


如果$A$存在逆矩阵$A^{-1}$，则可以在方程两边乘以$A^{-1}$，则有：


$$
A^{-1}Ax=A^{-1}y
$$


可以得到：


$$
Ix=A^{-1}y
$$


因为$Ix=x$，最终可以得到：


$$
x=A^{-1}y
$$



需要注意，并不是所有矩阵都是可逆的（线性方程组可能无解）。

在`numpy`中，使用`.linalg.inv`计算：

```python
A = np.array([[1, 2, 1],
              [4, 4, 5],
              [6, 7, 7]])

A_i = np.linalg.inv(A)
>>> A_i
array([[-7., -7.,  6.],
       [ 2.,  1., -1.],
       [ 4.,  5., -4.]])

```

验证一下：

```python
>>> np.round(A @ A_i )
array([[ 1.,  0.,  0.],
       [-0.,  1.,  0.],
       [-0.,  0.,  1.]])
```



#### 2.2.6 矩阵的转置（matrix transpose）

将一个矩阵$A\in {\Bbb R}^{m \times n}$沿着对角线翻转就可以得到转置矩阵$A^T \in {\Bbb R}^{n \times m}$，满足以下关系：


$$
(A^T)_{ij} = A_{ji}
$$


例如：


$$
\begin{bmatrix}
1 &3 &5  \\
2 &4 &6 \\
\end{bmatrix}
^T=
\begin{bmatrix}
1 &2   \\
3 &4  \\
5 &6  \\
\end{bmatrix}
$$



在`numpy`中，使用`T`计算：

```python
A = np.array([[1, 3, 5],
              [2, 4, 6]])

>>> A.T
array([[1, 2],
       [3, 4],
       [5, 6]])
```



#### 2.2.7 哈达玛积（Hadamard product）

需要注意，在上文中提到了矩阵与矩阵的乘法，计算规则如下：


$$
c_{ij} :=\sum_{k=1}^n a_{ik}b_{kj},i=1,\dots m, \ j=1,\dots p
$$


而哈达玛积更简单，它遵循逐个元素操作的原则（element-wise），两个矩阵$A\in {\Bbb R}^{m \times n}$和$B\in {\Bbb R}^{m \times n}$，它们的哈达玛积$C\in {\Bbb R}^{m \times n}=A\odot B$，满足


$$
c_{ij}:= a_{ij} \cdot b_{ij}
$$


例如


$$
A\odot B=
\begin{bmatrix}
1 &2   \\
3 &4  \\
\end{bmatrix}
\begin{bmatrix}
5 &6   \\
7 &8  \\
\end{bmatrix}
=
\begin{bmatrix}
1\times 5 &2\times 6   \\
3\times 7 &4 \times 8  \\
\end{bmatrix}
=
\begin{bmatrix}
5 &12   \\
21 &32  \\
\end{bmatrix}
$$


在`numpy`中，使用`*`或`multiply`计算：

```python
A = np.array([[1, 2],
              [3, 4]])

B = np.array([[5, 6],
              [7, 8]])

>>> A*B
array([[ 5, 12],
       [21, 32]])
>>> np.multiply(A,B)
array([[ 5, 12],
       [21, 32]])
```



### 2.2 用矩阵方法求解线性方程组

下面是一个方程组，我们需要求解$x,y,z$：


$$
\begin{align*}
x-2y+3z = 9 \\
-x+3y =-4\\
2x-5y+5z=17
\end{align*}
$$


转化为线性方程组：


$$
\begin{bmatrix}
1 &-2 &3 \\
-1 &3 &0 \\
2 &-5 &5 
\end{bmatrix}
\begin{bmatrix}
x \\
y \\
z
\end{bmatrix}
=
\begin{bmatrix}
9 \\
-4 \\
17
\end{bmatrix}
$$


转为增广矩阵（augmented matrix）：


$$
\begin{bmatrix}

\begin{array}{ccc|c}
1 &-2 &3 &9\\
-1 &3 &0 &-4\\
2 &-5 &5 &17

\end{array}
    
\end{bmatrix}
$$

#### 2.2.1 高斯消元（Gaussian Elimination）

高斯消元法（gaussian elimination）是求解线性方程组的一种方法，他的目标是将线性方程组的增广矩阵转化为行阶梯矩阵（Row-Echelon Form）的形式求解：


$$
\begin{bmatrix}

\begin{array}{ccc|c}
1 &a &b &d \\
0 &1 &c &e \\
0 &0 &1 &f \\
\end{array}
    
\end{bmatrix}
$$


**基本的行操作方法：**

1. 交换两行：记为$R_i\leftrightarrow R_j$
2. 将行乘以非0数：记为$R_i \rightarrow NR_j$
3. 将两行相加赋予另一列：记为$R_i+R_j \rightarrow R_j$

**具体操作步骤**：

<img src="https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210715174405.png?x-oss-process=style/wp" style="zoom:50%;" />

1. 使第1行，第1列变成1
2. 用1将第1列的其他位置变成0
3. 使第2行，第2列变成1
4. 用1将第2列的其他位置变成0
5. 使第3行，第3列变成1
6. 以此类推

例如，求解上文说的方程组：


$$
\begin{align*}
x-2y+3z = 9 \\
-x+3y =-4\\
2x-5y+5z=17
\end{align*}
$$


具体步骤如下：


$$
\begin{align*}
\begin{bmatrix}

\begin{array}{ccc|c}
1 &-2 &3 &9\\
-1 &3 &0 &-4\\
2 &-5 &5 &17

\end{array}
    
\end{bmatrix}

\ \ 
R_1+R_2 \rightarrow R_2

\begin{bmatrix}

\begin{array}{ccc|c}
1 &-2 &3 &9\\
0 &1 &3 &5\\
2 &-5 &5 &17

\end{array}
    
\end{bmatrix}
\\ 
-2R_1+R_3 \rightarrow R_3

\begin{bmatrix}

\begin{array}{ccc|c}
1 &-2 &3 &9\\
0 &1 &3 &5\\
0 &-1 &-1 &-1

\end{array}
    
\end{bmatrix}

\\
R_2+R_3 \rightarrow R_3

\begin{bmatrix}

\begin{array}{ccc|c}
1 &-2 &3 &9\\
0 &1 &3 &5\\
0 &0 &2 &4

\end{array}
    
\end{bmatrix}

\\  
\frac{1}{2}R_3 \rightarrow R_3

\begin{bmatrix}

\begin{array}{ccc|c}
1 &-2 &3 &9\\
0 &1 &3 &5\\
0 &0 &1 &2

\end{array}
    
\end{bmatrix}
\end{align*}
$$


我们可以得到一个新的方程组：


$$
\begin{align*}
x-2y+3z = 9 \\
y + 3z =5 \\
z=2
\end{align*}
$$


所以：


$$
\begin{align*}
x=1 \\
y=-1 \\
z=2
\end{align*}
$$


在`numpy`中用`linalg.solve`求解：

```python
import numpy as np
A = np.array([
    [1,-2,3],
    [-1,3,0],
    [2,-5,5]    
])
y = np.array([
    [9],
    [-4],
    [17]
])

>>> np.linalg.solve(A,y)
array([[ 1.],
       [-1.],
       [ 2.]])
```



#### 2.2.2 高斯-约旦消元

基于高斯消元法可以得到：


$$
\begin{bmatrix}

\begin{array}{ccc|c}
1 &a &b &d \\
0 &1 &c &e \\
0 &0 &1 &f \\
\end{array}
    
\end{bmatrix}
$$


再前进一步，将它修改为：


$$
\begin{bmatrix}

\begin{array}{ccc|c}
1 &0 &0 &a \\
0 &1 &0 &b \\
0 &0 &1 &c \\
\end{array}
    
\end{bmatrix}
$$


我们就可以直接得到：


$$
\begin{align*}
x = a \\
y = b \\
z = c
\end{align*}
$$



这便是高斯-约旦消元法，具体步骤可参考下图：

<img src="https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210715213550.png?x-oss-process=style/wp" style="zoom:50%;" />



### 2.2 四种基本的矩阵子空间

#### 2.2.1 基

假设$V$是${\Bbb R}^n$的子空间，$V$的基是指一组向量$\{v_1,v_2,\cdots,v_m\}$，满足：

1. $V = Span\{v_1,v_2,\cdots,v_m\}$
2. $\{v_1,v_2,\cdots,v_m\}$线性无关

例如${\Bbb R}^2$的一个基是
$$
\{ 
\begin{bmatrix}
1 \\ 0
\end{bmatrix},
\begin{bmatrix}
0 \\ 1
\end{bmatrix}
\}
$$
因为${\Bbb R}^2$空间内任意一个向量 $\begin{bmatrix}
a \\ b
\end{bmatrix}$可以写成以下形式：
$$
\begin{bmatrix}
a \\ b
\end{bmatrix}
=
a
\begin{bmatrix}
1 \\ 0
\end{bmatrix}
+
b
\begin{bmatrix}
0 \\ 1
\end{bmatrix}
$$
并且$\begin{bmatrix}
1 \\ 0
\end{bmatrix},
\begin{bmatrix}
0 \\ 1
\end{bmatrix}$是线性无关的，因为使下列等式成立的唯一条件是$x=y=0$
$$
\begin{bmatrix}
0 \\ 0
\end{bmatrix}
=
x
\begin{bmatrix}
1 \\ 0
\end{bmatrix}
+
y
\begin{bmatrix}
0 \\ 1
\end{bmatrix}
$$

> 请参见下文了解如何寻找矩阵列空间的基

#### 2.2.1 列空间、行空间

矩阵$A$的列空间是由$A$的所有列的线性组合构成，记作$C(A)$，也就是$C(A)$等于$A$列的生成空间（span），例如
$$
A=
\begin{bmatrix}
1 &0 &2 \\
0 &-1 &3
\end{bmatrix}
\\
$$
则：
$$
C(A) = 
\alpha 
\begin{bmatrix}
1 \\
0
\end{bmatrix}
+ \beta
\begin{bmatrix}
0 \\
-1
\end{bmatrix}
+ \gamma
\begin{bmatrix}
2 \\
3
\end{bmatrix}，其中\alpha,\beta,\gamma \in {\Bbb R}
$$
可以写作：
$$
C(A) = \Bigg \{ 
\begin{bmatrix}
1 &0 &2\\
0 &-1 &3
\end{bmatrix}
\begin{bmatrix}
\alpha \\
\beta \\
\gamma
\end{bmatrix}
:\alpha,\beta,\gamma \in {\Bbb R}

\Bigg \}
$$
所以，如果向量 $\begin{bmatrix} a \\ b \end{bmatrix}$在列空间$C(A)$中，则说明下列线性方程组有解：
$$
\begin{bmatrix}
1 &0 &2\\
0 &-1 &3
\end{bmatrix}
\begin{bmatrix}
\alpha \\
\beta \\
\gamma
\end{bmatrix}
=
\begin{bmatrix}
a \\
b
\end{bmatrix}
$$
类似的，矩阵的行空间$R(A)$可以通过$C(A^T)$计算。

#### 2.2.2 计算列空间的基

计算下列矩阵的列空间的基：
$$
A=
\begin{bmatrix}
1 &-2 &3 &1 \\
0 &1 &-1 &0 \\
-1 &1 &-2 &-1 \\
\end{bmatrix}
\\
$$
根据列空间的定义，可以得到：
$$
C(A)=
\alpha 
\begin{bmatrix}
1 \\
0 \\
-1
\end{bmatrix}
+ \beta
\begin{bmatrix}
-2 \\
1 \\
1
\end{bmatrix}
+ \gamma
\begin{bmatrix}
3 \\
-1 \\
-2
\end{bmatrix}
+ \delta
\begin{bmatrix}
1 \\
0 \\
-1
\end{bmatrix}
，其中\alpha,\beta,\gamma ,\delta \in {\Bbb R}
$$
这符合基的第一个定义：
$$
V = Span\{v_1,v_2,\cdots,v_m\}
$$
但是，下列这些向量之间线性无关吗？
$$
\begin{bmatrix}
1 \\
0 \\
-1
\end{bmatrix}
,
\begin{bmatrix}
-2 \\
1 \\
1
\end{bmatrix}
,
\begin{bmatrix}
3 \\
-1 \\
-2
\end{bmatrix}
,
\begin{bmatrix}
1 \\
0 \\
-1
\end{bmatrix}
$$
即仅当$\alpha,\beta,\gamma ,\delta $都为0时才满足下列等式？
$$
\bold 0=
\alpha 
\begin{bmatrix}
1 \\
0 \\
-1
\end{bmatrix}
+ \beta
\begin{bmatrix}
-2 \\
1 \\
1
\end{bmatrix}
+ \gamma
\begin{bmatrix}
3 \\
-1 \\
-2
\end{bmatrix}
+ \delta
\begin{bmatrix}
1 \\
0 \\
-1
\end{bmatrix}
$$
显然不是，至少$A_1 = A_4$，说明我们需要简化$A_1,A_2,A_3,A_4$。

使用上面的高斯消元法将矩阵$A$简化成行阶梯矩阵，以便看出列之间的关系：
$$
\begin{bmatrix}
1 &-2 &3 &1 \\
0 &1 &-1 &0 \\
-1 &1 &-2 &-1 \\
\end{bmatrix}
\\
\rightarrow

\begin{bmatrix}
1 &0 &1 &1 \\
0 &1 &-1 &0 \\
0 &0 &0 &0 \\
\end{bmatrix}
$$
可以看到$A_3 = A_1-A_2,A_4= A_1$，所以列空间$C(A)$的基为$A_1,A_2$



#### 2.2.2 秩（rank）

**定理：**

假设$V$是${\Bbb R}^n$的一个子空间，$V$的任意一个基（basis）的向量的个数是一样的，称为$V$的维度（dimension），记为$dim \  V$。

矩阵$A$的列空间和行空间拥有相同的维度，即$dim(C(A)) = dim(R(A)) $。

矩阵$A$的秩（rank），记为$rank(A)$，是指$A$的列空间或行空间的维度。

上面$A=
\begin{bmatrix}
1 &-2 &3 &1 \\
0 &1 &-1 &0 \\
-1 &1 &-2 &-1 \\
\end{bmatrix}
\\$的例子中，我们可以知道$rank(A)=2$。

#### 2.2.3 零空间

矩阵$A$的零空间是是指满足$A v=\bold 0$所有向量$v$的集合，记作$N(A)$。

### 2.3 矩阵范数（matrix norm）

#### 2.3.1 F-范数（Frobenius norm）

对于矩阵$A^{m \times n}$，它的F-范数$||A||_F$定义如下：
$$
||A||_F :=\sqrt{\sum_{i=1}^m \sum_{j=1}^n a_{ij}^2}
$$


如：
$$
\begin{align}
A = 
\begin{bmatrix}
1 &2 &3 \\
4 &5 &6 \\
7 &8 &9
\end{bmatrix}
, 

||A||_F &= \sqrt{1^2+2^2+3^2+4^2+5^2+6^2+7^2+8^2+9^2} \\
&=16.881943016134134
\end{align}
$$


在`numpy`中，使用`np.linalg.norm`计算：

```python
A = np.array([[1, 2, 3],
              [4, 5, 6], 
              [7, 8, 9]])
>>> np.linalg.norm(A,'fro')
16.881943016134134
```



#### 2.3.2 最大值范数（Max norm）

对于矩阵$A^{m \times n}$，它的最大值范数$||A||_{ax}$定义如下：
$$
||A||_{ax} :=max_i \sum_{j=1}^n |a_{ij}|
$$
算法如下：

1. 计算每行元素绝对值的和；
2. 找出最大值得和作为范数。



如：
$$
A = 
\begin{bmatrix}
1 &2 &3 \\
-4 &5 &6 \\
7 &-8 &9
\end{bmatrix}
$$


的最大值范数$7 + |-8|+9 = 24$。

在`numpy`中，使用`np.linalg.norm`计算：

```python
A = np.array([[1, 2, 3],
              [-4, 5, 6], 
              [7, -8, 9]])

>>> np.linalg.norm(A,np.inf)
24.0
```



#### 2.3.3 谱范数（spectral norm）

待完善。



## 三、线性映射与仿射映射（linear and affine mapping）

### 3.1 线性映射

假设一个线性映射函数`T`和一组向量`x,y`，需要满足以下条件：
$$
T(x+y) = T(x) + T(y) \\
T(\alpha x) = aT(x),\forall \alpha
$$

即：

* 分配率：对一组向量和的转化效果要等于分别对这两个向量处理结果的和；
* 结合律

这两个性质可以统一成一个叠加原理（superposition property）：
$$
T(\alpha x + \alpha y) = \alpha T(x) + \alpha T(y)
$$
在线性代数中，线性映射可以用矩阵乘法来表达：
$$
T(x) = Ax
$$

### 3.2 仿射映射

待完善。

### 3.3 各种特殊的线性映射

可以把线性映射想象成平面内的坐标系变化，原坐标系相应的点也会跟着变化:

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/imgLinear%20transformations%20and%20matrices%20_%20Chapter%203,%20Essence%20of%20linear%20algebra-kYB8IZa5AuE.2021-07-30%2010_55_57.gif)

> 动画来源：[YouTube 3brown1blue](https://www.youtube.com/watch?v=kYB8IZa5AuE&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab&index=3&ab_channel=3Blue1Brown)

假设一个矢量为$[2,2]^T$​​​，对他进行线性变换，那最终这个点会落在哪里呢？
$$
\begin{bmatrix}
-1 &0 \\
0 &1
\end{bmatrix}
\begin{bmatrix}
2 \\
2
\end{bmatrix}
=?
$$
我们当然可以计算，但是可以用另一种思维去理解：

1. 笛卡尔坐标系中基底矢量分别是$[1,0]^T$​和$[0,1]^T$，即横坐标和纵坐标上距离为1的位置：

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210730111800.png?x-oss-process=style/wp)

2. 空间中任何点都可以用这两个基底矢量表示，如

$$
\begin{bmatrix}
2 \\
2
\end{bmatrix}
=
2
\begin{bmatrix}
1 \\
0 
\end{bmatrix}
+
2
\begin{bmatrix}
0 \\
1
\end{bmatrix}
$$

它可以写成：
$$
\begin{bmatrix}
2 \\
2
\end{bmatrix}
=

\begin{bmatrix}
1 & 0\\
0 & 1
\end{bmatrix}

\begin{bmatrix}
2 \\
2
\end{bmatrix}
$$
$\begin{bmatrix}
1 & 0\\
0 & 1
\end{bmatrix}$​​​ 这部分的第1列和第2列就是坐标系的基底矢量，如果这部分变成了$\begin{bmatrix} -1 & 0\\
0 & 1
\end{bmatrix}$​​，可以想象原来$x$​轴上的$(1,0)$​变化至$(-1,0)$​，而$y$轴不变，还是$(0,1)$​，这相当于沿着$y$轴做镜像：

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210730111833.png?x-oss-process=style/wp)

3. 想象一下这个坐标系上原来的点跟着变化，那么$[2,2]^T$也会变成$[-2,2]^T$

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210730112201.png?x-oss-process=style/wp)

更普遍的，$\begin{bmatrix}
1 & 0\\
0 & 1
\end{bmatrix}$变化至$\begin{bmatrix}
x_1 & x_2\\
y_1 & y_2 \end{bmatrix}$​​​​​​​​​，就想象成这个坐标系上所有的矢量随着基底$[1,0]^T ,[0,1]^T$ 变化至$[x_1,y_1]^T,[x_2,y_2]^T$一起变化：

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/imgLinear%20transformations%20and%20matrices%20_%20Chapter%203,%20Essence%20of%20linear%20algebra-kYB8IZa5AuE.2021-07-30%2010_55_57.gif)



下面分别介绍一些特殊的变化。

#### 3.3.1 缩放（scaling）

使用$y=Ax,A=\alpha I$进行缩放，其中：

* $\alpha >1$时，将$x$拉伸放大
* $\alpha <1$时，将$x$压缩缩小
* $\alpha < 0$时，倒置矢量$\vec x$

如一个大小为${\Bbb R}^2$的缩放矩阵：
$$
\begin{bmatrix}
s_1 &0 \\
0 &s_2
\end{bmatrix}
$$
其中$s_1,s_2$为缩放因子（scaling factors），如：
$$
x  = 
\begin{bmatrix}
0 & 2 \\
1 & 4
\end{bmatrix}
,
A =
\begin{bmatrix}
3 & 0 \\
0 & 3
\end{bmatrix}
$$
则缩放后为：
$$
y = Ax = 
\begin{bmatrix}
0 & 6 \\
3 & 12
\end{bmatrix}
$$
在`numpy`中验证：

```python
import numpy as np
x = np.array([
    [0,2],
    [1,4]
])
A = np.array([
    [3,0],
    [0,3]
])
>>> A @ x
array([[ 0,  6],
       [ 3, 12]])
```

#### 3.3.2 镜像（reflection）

如下图，在笛卡尔坐标系中，一条穿过坐标原点的线（图中绿线）与横坐标夹角为$\theta$，矢量$\vec a$ 相对于绿线的镜像为$\vec a'$

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210722115529.png?x-oss-process=style/wp)

则有


$$
\vec a' = 
\begin{bmatrix}
cos(2\theta) &sin(2\theta) \\
sin(2\theta) &-cos(2\theta)
\end{bmatrix}
\vec a
$$


1. 以横坐标$x$做镜像：
$$
\vec a' = 
 \begin{bmatrix}
 1 &0 \\
 0 &-1
 \end{bmatrix}
 \vec a
$$


2. 以纵坐标$y$做镜像：

$$
\vec a' = 
\begin{bmatrix}
-1 &0 \\
0 &1
\end{bmatrix}
\vec a
$$

3. 沿着$y=x$的直线（$\theta=45^{\circ}$）做镜像：

$$
\vec a' = 
\begin{bmatrix}
0 &1 \\
1 &0
\end{bmatrix}
\vec a
$$

4. 沿着$y=-x$的直线（$\theta=-45^{\circ}$）做镜像：

$$
\vec a' = 
\begin{bmatrix}
0 &-1 \\
-1 &0
\end{bmatrix}
\vec a
$$

如$\vec a=[0,2]^T$​，则分别有：


$$
\begin{align}

&1.横坐标镜像：
\vec a' =
 \begin{bmatrix}
 1 &0 \\
 0 &-1
 \end{bmatrix}
\begin{bmatrix}
0 \\
2
\end{bmatrix}
=
\begin{bmatrix}
0 \\
-2
\end{bmatrix}
\\ \\
&2.纵坐标镜像：
\vec a' =
 \begin{bmatrix}
 -1 &0 \\
 0 &0
 \end{bmatrix}
\begin{bmatrix}
0 \\
2
\end{bmatrix}
=
\begin{bmatrix}
0 \\
2
\end{bmatrix}
\\ \\
&3.\theta=45^{\circ}镜像：
\vec a' =
 \begin{bmatrix}
0 &1 \\
1 &0
 \end{bmatrix}
\begin{bmatrix}
0 \\
2
\end{bmatrix}
=
\begin{bmatrix}
2 \\
0
\end{bmatrix}
\\ \\
&4.\theta=-45^{\circ}镜像：
\vec a' =
 \begin{bmatrix}
0 &-1 \\
-1 &0
 \end{bmatrix}
\begin{bmatrix}
0 \\
2
\end{bmatrix}
=
\begin{bmatrix}
-2 \\
0
\end{bmatrix}
\end{align}
$$


在`numpy`中验证：

```python
# rotation along the horiontal axis
A1 = np.array([[1.0, 0],
               [0, -1.0]])

# rotation along the vertical axis
A2 = np.array([[-1.0, 0],
               [0, 1.0]])

# rotation along the line at 45 degrees from the origin
A3 = np.array([[0, 1.0],
               [1.0, 0]])

# rotation along the line at -45 degrees from the origin
A4 = np.array([[0, -1.0],
               [-1.0, 0]])

x = np.array([
    [0],
    [2]
])

>>> A1 @ x
array([[ 0.],
       [-2.]])

>>> A2 @ x
array([[0.],
       [2.]])

>>> A3 @ x
array([[2.],
       [0.]])

>>> A4 @ x
array([[-2.],
       [ 0.]])
```

#### 3.3.3 错切（shear）

如图，错切可以简单理解成将矩形变为平行四边形的过程。

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210722151945.png?x-oss-process=style/wp)

在笛卡尔坐标系中，矢量$\vec a\in{\Bbb R}^2$ 沿着横坐标错切后的$\vec a'$为：


$$
\vec a' = 
\begin{bmatrix}
1 &m \\
0 &1
\end{bmatrix}
\vec a
$$
其中$m$是错切因子（shear factor），它决定了错切的程度，如上图图分别是$m=1$和$m=2$的错切后效果图（想象一下，把图片上每个点都做错切）。



矢量$\vec a\in{\Bbb R}^2$ 沿着纵坐标坐标错切后的$\vec a'$为：
$$
\vec a' = 
 \begin{bmatrix}
 1 &0 \\
 m &1
 \end{bmatrix}
 \vec a
$$
下图分别是$m=1$和$m=2$的错切后效果图

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210722152241.png?x-oss-process=style/wp)

#### 3.3.3 旋转（rotation）

在笛卡尔坐标系中，矢量$\vec a\in{\Bbb R}^2$ 围绕坐标原点逆时针旋转$\theta$后的$\vec a'$​为：


$$
\vec a' = 
\begin{bmatrix}
cos\theta &-sin\theta \\
sin\theta &cos\theta
\end{bmatrix}
\vec a
$$



如逆时针旋转$90^\circ$​：


$$
\vec a' = 
\begin{bmatrix}
0 &-1 \\
1 &0

\end{bmatrix}
\vec a
$$


效果图如下：

<img src="https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210722160716.png?x-oss-process=style/wp" style="zoom:50%;" />

### 3.4 投影（projections）

如图，向量$\bold b$​​​到向量$\bold a$​​​最短的距离是从$b$​​​向$a$​​​画一个垂直线，交叉点为$\bold p$​​​：

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210803165926.png?x-oss-process=style/wp)



向量$\bold p$​可看成$\bold b$​的近似，而$\bold {e=b-p}$​则是相似度的误差。假设$\bold p=x\bold a$​​​，根据向量的正交性可以得到$\bold a \perp (\bold b - x\bold a)$：


$$
\begin{align*}
\bold a ^T (\bold b -x \bold a) &= 0 \\
\bold a ^T \bold b &= x\bold a^T\bold a \\
x &= \frac{\bold a ^T \bold b }{\bold a^T\bold a }
\end{align*}
$$
而$\bold p=x\bold a = \bold a \frac{\bold a ^T \bold b }{\bold a^T\bold a }$​​。



#### 3.4.1 投影矩阵（projection matrix）

假设有一个矩阵$p$​满足$\bold p = p \bold b$​​，我们称$p$为投影矩阵，因为：


$$
\bold p = x \bold a = \bold a \frac{a^Tb}{a^Ta}
$$
所以投影矩阵$p$有：
$$
p = \bold a \frac{a^T}{a^Ta} = \frac{a a^T}{a^Ta}
$$

> 需要注意$a^T a \neq a a^T$​​​，因为矢量$a\in {\Bbb R}^{n\times1}$​​​，则有$a^T\in {\Bbb R}^{1\times n}$，则$a^T a$是一个标量（scalar），而$a a ^T \in {\Bbb R}^{n\times n}$​​​，这是一个矩阵。



这个投影矩阵$p$可以将任何矢量$\bold b$投影到$\bold a$上，因为投影$\bold p$已经在$\bold a$上，所以再对它投影得到的效果是一样的，即$p^2\bold b=p\bold b$​​，即幂等性（idempotent）：
$$
p^T = p
$$

#### 3.4.2 为什么需要投影？

#### 3.4.3 更高维度的投影

