---
layout: post
title: '机器学习相关线性代数简介（持续完善中）'
date: 2021-07-12
tags:
  线性代数
  linear-algebra
  data-science
  vector
  matrix
---
> 参考资料：
>
> * [mathematics for machine learning](https://mml-book.github.io/)
> * [Introduction to Linear Algebra for Applied Machine Learning with Python](https://pabloinsente.github.io/intro-linear-algebra)
> * [Matrices: Gaussian & Gauss-Jordan Elimination](https://www.craftonhills.edu/current-students/tutoring-center/mathematics-tutoring/matrices-gauss-jordan.pdf)
> * [Transformations and Matrices](https://www.mathsisfun.com/algebra/matrix-transform.html)
> * [The Geometry of the Dot and Cross Products](https://www.maa.org/sites/default/files/images/images/upload_library/4/vol6/Dray2/Dray.pdf)
> * 起始编辑时间：2021-07-12

[toc]



## 一、向量（vectors）

### 1.1类型

#### 1.1.1 几何向量：

我们高中时学到、见到的最多的类型，一般在二维或三维中作图就可以表示

<img src="https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210712154844.png?x-oss-process=style/wp" style="zoom:50%;" />



#### 1.1.2 多项式：

多项式如$f(x)=x^2+y+1$，之所以也称为向量是因为它满足向量的定义：

* 可以用加法，得到新的多项式；
* 可以用乘法，得到新的多项式；

如$f(x)+g(x)$和$5\times f(x)$

<img src="https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210712155230.png?x-oss-process=style/wp" style="zoom:50%;" />



#### 1.1.3 实数集合：

广义地说，任意实数组成的集合 ${\Bbb R}^n$ 也是向量，这是机器学习中最常见、最重要的向量：



$$
X = 
\begin{bmatrix}
x_1 \\
x_2 \\
. \\
. \\
. \\
x_n
\end{bmatrix}
\in 
{\Bbb R}^n
$$


如，一个$  {\Bbb R}^3$ 3维的向量：


$$
x = 
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
\end{bmatrix}
\in
{\Bbb R}^3
$$



### 1.2向量的基本运算

#### 1.2.1 加法

两个大小一样的向量的加法，需要向量的每个元素逐个相加（element-wise）：


$$
x+y =
\begin{bmatrix}
x_1 \\
. \\
. \\
. \\
x_n
\end{bmatrix}
+
\begin{bmatrix}
y_1 \\
. \\
. \\
. \\
y_n
\end{bmatrix}
=
\begin{bmatrix}
x_1 + y_1 \\
. \\
. \\
. \\
x_n + y_n
\end{bmatrix}
$$


如


$$
x+y = 
\begin{bmatrix}
1 \\
2 \\
3 \\
\end{bmatrix}
+
\begin{bmatrix}
3 \\
4 \\
5 \\
\end{bmatrix}
=
\begin{bmatrix}
1 + 3 \\
2 + 4 \\
3 + 5
\end{bmatrix}
=
\begin{bmatrix}
4 \\
6 \\
8 
\end{bmatrix}
$$


向量加法满足以下性质：

* 交换律（Commutativity）：$x + y = y + x$
* 结合律（Associativity）：$(x+y)+z=x+(y+z)$
* 零向量$\vec{0}$无效：$x+0 = x$
* 减去自身等于零向量$\vec{0}$：$x-x = \vec{0}$

在`numpy`中，使用运算符`+`或`add`方法计算两个向量的和

```python
import numpy as np
# 赋值x,y为向量[1,2,3]
x = y = np.array([[1],
                  [2],
                  [3]])
>>> x + y
array([[2],
       [4],
       [6]])

>>> np.add(x,y)
array([[2],
       [4],
       [6]])
```

#### 1.2.2向量与标量相乘（vector-scalar multiplication）

向量与标量的乘法也遵循逐个元素操作的原则（element-wise）：


$$
ax = 
\begin{bmatrix}
ax_1 \\
. \\
. \\
. \\
ax_n \\
\end{bmatrix}
$$



假设$a=3$，$x=[1,2,3]^T$，则


$$
ax = 3
\begin{bmatrix}
1 \\
2 \\
3 \\
\end{bmatrix}
=
\begin{bmatrix}
3\times1 \\
3\times2 \\
3\times3 \\
\end{bmatrix}
=
\begin{bmatrix}
3 \\
6 \\
9 \\
\end{bmatrix}
$$


向量与标量的乘法满足以下性质：

* 结合律（Associativity）：$(\alpha\beta)x=\alpha(\beta x)$
* 左分配率（Left-distributive）：$(\alpha+\beta)x=\alpha x+ \beta x$
* 右分配率（Right-distributive）：$x(\alpha+\beta)=x\alpha + x\beta $
* 其他：$\alpha(x+y)=\alpha x + \alpha y$

在`numpy`中，使用运算符`*`或`np.multiply`计算向量与标量乘积

```python
import numpy as np

x = 3
y = np.array([
    [3],
    [4],
    [5],
    [6]
])

>>> x*y
array([[ 9],
       [12],
       [15],
       [18]])
```

#### 1.2.3向量的线性组合

$$
\alpha x  + \beta y = \alpha 
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
+
\beta
\begin{bmatrix}
y_1 \\
y_2
\end{bmatrix}
=
\begin{bmatrix}
\alpha x_1+ \beta y_1 \\
\alpha x_2 + \beta y_2
\end{bmatrix}
$$



如 $\alpha =2 , \beta=3,x=[2,3]^T,y=[4,5]^T$，则


$$
\alpha x + \beta y = 
2 
\begin{bmatrix}
2 \\
3
\end{bmatrix}
+
3
\begin{bmatrix}
4 \\
5
\end{bmatrix}
=
\begin{bmatrix}
2 \times 2 + 3 \times 4 \\
2 \times 3 + 3 \times 5
\end{bmatrix}
=
\begin{bmatrix}
16 \\
21
\end{bmatrix}
$$


线性组合的另一种方式是用求和公式，假设$x_i,...x_k$是一个向量，标量集合$\beta_i,...,\beta_k\in {\Bbb R}$，则


$$
\sum_{i=1}^k \beta_i x_i := \beta_1x_1 + ...+\beta_kx_k
$$


公式中$:=$表示“定义为”的意思。

在`numpy`中，计算向量的线性组合

```python
import numpy as np
a,b = 2,3
x,y = np.array([[2],[3]]), np.array([[4],[5]])

>>> a * x + b * y
array([[16],
       [21]])
```



### 1.3 向量空间（vector space）

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210712212634.png?x-oss-process=style/wp)



向量空间${\Bbb R}^n$由所有的$n$维向量$\vec v$组成，

### 1.4向量子空间



### 1.5 向量范数（vector norms）

在机器学习算法中，经常使用到向量的范数，范数可以简单地理解为向量的长度（从原点到终点），如图，一般常用的范数有以下几种：

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210713165010.png?x-oss-process=style/wp)



#### 1.5.1 欧几里得范数（Euclidean norm）

欧几里得范数$L_2$最常用，它的定义为：


$$
||x||_2 := \sqrt{\sum_{i=1}^n x_i^2}
$$


如，二维向量的$L_2$范数为：


$$
||x||_2 \in {\rm I\!R} ^2 = \sqrt{x_1^2+x_2^2}
$$


在`numpy`中，用`np.linalg.norm`计算范数：

```python
import numpy as np
a = np.array([[3],[4]])
>>> np.linalg.norm(a,2)
5.0
```

上图第一个坐标图表达的是$L_2$范数为1的二维向量形成的区域，可以看到它是一个圆，之所以是一个圆是因为：



假设向量$a = [x,y]$是一个二维向量，根据$L_2$范数定义：


$$
||a||_2=\sqrt{x^2+y^2}
$$


正好$x,y$分别是向量在坐标系中的横坐标和纵坐标，所以$L_2$长度正好是三角形的斜边。



<img src="https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210713172736.png?x-oss-process=style/wp" style="zoom:50%;" />



#### 1.5.2 曼哈顿范数（Manhattan norm）

曼哈顿范数$L_1$，定义如下：


$$
||x||_1 :=\sum_{i=1}^n |x_i|
$$



如，二维向量的$L_1$范数为：


$$
||x||_1 \in {\rm I\!R} ^2 = |x_1|+|x_2|
$$


在`numpy`中，用`np.linalg.norm`计算范数：

```
import numpy as np
a = np.array([[3],[-4]])
>>> np.linalg.norm(a,1)
7.0
```

如下图，是$L_1$范数为1的二维向量形成的区域，可以看到它是一个方形（之所以叫曼哈顿范数，是因为$L_1$类似曼哈顿的网格型地形）。

<img src="https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210713180519.png?x-oss-process=style/wp" style="zoom: 50%;" />

如下图，之所以$L_1$之所以是一个方形是因为：

假设向量$a = [x,y]$是一个二维向量，根据$L_1$范数定义


$$
||a||_1=|x|+|y|
$$


正好$x,y$分别是向量在坐标系中的横坐标和纵坐标，所以$L_1$长度正好是三角形的两个直角边的长度和，又因为$y=z$（等腰直角三角形），所以


$$
L_1=|x|+|y|=|x|+|z|=1
$$


<img src="https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210713181531.png?x-oss-process=style/wp" style="zoom:50%;" />



#### 1.5.3 最大值范数（Max norm）

最大值范数很简单，就是取向量元素绝对值最大的值即可：


$$
||x||_{\infty} :=  max_i|x_i|
$$

例如，假设向量$x = [3,-4,-5]$的
$$
||x||_\infty=5
$$


在`numpy`中，用`np.linalg.norm`计算范数：

```python
import numpy as np
a = np.array([[3],[-4],[-5]])
>>> np.linalg.norm(a,np.inf)
5.0
```

### 1.6 向量内积（点乘），长度和距离（inner product,length and distance）

两个向量$x,y \in{\Bbb R}^n$的内积$ \langle x,y  \rangle$​定义如下：
$$
\langle x,y \rangle := x \cdot y = \sum_{i=1}^n x_i y_i
$$
即对两个向量执行对应位一一相乘再求和。



#### 1.6.1内积的几何意义

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210723164147.png?x-oss-process=style/wp)

如图，经过证明可以得到，即两个向量的内积（内乘）可以计算两个向量的夹角。
$$
\vec A \cdot \vec B = |\vec A||\vec B|cos\theta
$$
证明过程如下：
$$
\begin{align*}
\rightarrow &1.证明 \ \vec C \cdot \vec C = |\vec C|^2 \\ \\
&假设\ \vec C = 
\begin{bmatrix}
x_1 \\
y_1
\end{bmatrix}
，则\ \vec C \cdot \vec C = 
\begin{bmatrix}
x_1 \\
y_1
\end{bmatrix}
\cdot
\begin{bmatrix}
x_1 \\
y_1
\end{bmatrix} =
x_1^2+y_1^2，\\

&根据勾股定理有， |\vec C| = \sqrt {x_1^2+y_1^2}，所以有: \\
& |\vec C|^2 = (\sqrt {x_1^2+y_1^2})^2 = x_1^2+y_1^2 = \vec C \cdot \vec C
\\ \\
\rightarrow &2.证明\ (\vec A + \vec B) \cdot \vec C = \vec A \cdot \vec C +  \vec B \cdot \vec C
\\ \\
&假设 \
\vec A = 
\begin{bmatrix}
x_a \\
y_a
\end{bmatrix},
\vec B = 
\begin{bmatrix}
x_b \\
y_b
\end{bmatrix},
\vec C = 
\begin{bmatrix}
x_c \\
y_c
\end{bmatrix},则 
\\
&(\vec A + \vec B) \cdot \vec C = 
\bigg(
\begin{bmatrix}
x_a \\
y_a
\end{bmatrix}
+
\begin{bmatrix}
x_b \\
y_b
\end{bmatrix}
\bigg)
\cdot
\begin{bmatrix}
x_c \\
y_c
\end{bmatrix}
= \\

&\begin{bmatrix}
(x_a + x_b)\\
(y_a + y_b)
\end{bmatrix}
\cdot
\begin{bmatrix}
x_c \\
y_c
\end{bmatrix}
=
(x_a + x_b) x_c +
(y_a + y_b) y_c 
=
\\
&x_a x_c + x_b x_c + y_a y_c + y_b y_c =(x_a x_c +y_a y_c) + ( x_b x_c + y_b y_c) =
\\

&\begin{bmatrix}
x_a \\
y_a
\end{bmatrix}
\cdot
\begin{bmatrix}
x_c \\
y_c
\end{bmatrix}
+
\begin{bmatrix}
x_b \\
y_b
\end{bmatrix}
\cdot
\begin{bmatrix}
x_c \\
y_c
\end{bmatrix}
= \vec A \cdot \vec C +  \vec B \cdot \vec C
\\ \\
\rightarrow &3.证明 \vec A \cdot \vec B = |\vec A||\vec B|cos\theta
\\ \\
&因为\ \vec C \  = \vec B - \vec A，所以 |\vec C| ^2 = (-\vec A+ \vec B)(-\vec A+ \vec B) \\
&=\vec A \cdot  \vec A  + \vec B \cdot \vec B - 2\vec A \cdot \vec B = |\vec A|^2 + |\vec B|^2 -2\vec A \cdot \vec B，\\

&整理可得 |\vec C|^2 = |\vec A|^2+|\vec B|^2 -2\vec A \cdot \vec B。
\\
&又因为 三角形余弦定理可得 |\vec C|^2 = |\vec A|^2+|\vec B|^2 -2|\vec A|| \vec B|cos\theta
\\
&综上可得：\vec A\cdot \vec B = |\vec A| |\vec B|cos\theta
\end{align*}
$$




### 1.6 线性方程组（systems  of linear equations）

一般线性方程组如下：


$$
\begin{align*}
&x_1 &+ x_2 &+x_3 = 3 \\
&x_1 &- x_2 &+ 2x_3 = 2 \\
&2x_1 & &+ 3x_3=5 
\end{align*}
$$


将方程组系数$a_{ij}$挑出，组成向量（vectors）形式：


$$
\begin{bmatrix}
a_{11}\\
.\\
.\\
.\\
a_{m1}
\end{bmatrix}
x_1 
+ 
\begin{bmatrix}
a_{12}\\
.\\
.\\
.\\
a_{m2}
\end{bmatrix}
x_2

+ ... + 
\begin{bmatrix}
a_{1n}\\
.\\
.\\
.\\
a_{mn}
\end{bmatrix}
x_n
=
\begin{bmatrix}
b_1\\
.\\
.\\
.\\
b_m
\end{bmatrix}
$$



再将向量变成矩阵（matrices）形式：


$$
\begin{bmatrix}
&a_{11} &...&a_{1n}\\
&. & &. \\
&. & &. \\
&. & &. \\
&a_{m1} &...&a_{mn}
\end{bmatrix}
\begin{bmatrix}
x_1 \\
. \\
. \\
. \\
x_n
\end{bmatrix}
=
\begin{bmatrix}
b_1 \\
. \\
. \\
. \\
b_n
\end{bmatrix}
$$



## 二、矩阵（matrices）

一个有着$m$行$n$列的矩阵$A\in {\Bbb R}^{m \times n}$的定义如下：


$$
A :=
\begin{bmatrix}
&a_{11} & a_{12} &... &a_{1n} \\
&a_{21} & a_{12} &... &a_{2n} \\
&⋮ &⋮  &⋱ &⋮ \\
&a_{m1} & a_{m2} &... &a_{mn} \\

\end{bmatrix}
,
a_{ij} \in {\rm I\!R}
$$


在`numpy`中，构造一个矩阵：

```python
import numpy as np
A = np.array([
    [1,7],
    [3,5]
])

>>> A
array([[1, 7],
       [3, 5]])
```



### 2.1 基本运算

#### 2.1.1 加法

矩阵加法遵循逐个元素操作的原则（element-wise），矩阵$A\in{\Bbb R}^{m\times n}$和$B\in {\Bbb R}^{m\times n}$的和为：


$$
A+B :=
\begin{bmatrix}
&a_{11}+b_{11} & a_{12} + b_{12} &... &a_{1n} + b_{1n}\\
&a_{21}+b_{21} & a_{22} + b_{22} &... &a_{2n} + b_{2n}\\
&⋮ &⋮  &⋱ &⋮ \\
&a_{m1} +b_{m1} & a_{m2}+ b_{m2} &... &a_{mn} +b_{mn}\\

\end{bmatrix}
\in
{\rm I\!R}^{m\times n}
$$



例如：


$$
A=
\begin{bmatrix}
0 &2 \\
1 &4
\end{bmatrix},
B=
\begin{bmatrix}
3 &1 \\
-3 &2
\end{bmatrix}
,
则 \ A+B=
\begin{bmatrix}
3+0 &2+1 \\
1-3 &4+2
\end{bmatrix}
=
\begin{bmatrix}
3 &3 \\
-2 &6
\end{bmatrix}
$$



在`numpy`中，类似向量的操作，用`+`或`np.add`运算：

```python
A = np.array([
    [0,2],
    [1,4]
])
B = np.array([
    [3,1],
    [-3,2]
])

>>> A+B
array([[ 3,  3],
       [-2,  6]])
>>> np.add(A,B)
array([[ 3,  3],
       [-2,  6]])
```



#### 2.2.2矩阵与标量相乘（Matrix-scalar multiplication）

矩阵与标量的乘法也遵循逐逐个元素操作的原则（element-wise）：矩阵$A$与标量$a$相乘，使矩阵每个元素$a_{ij}$都乘以$a$，即$a_{ij}\times a$

如：


$$
2 \times 
\begin{bmatrix}
0 &2 \\
1 &4
\end{bmatrix} 
=
\begin{bmatrix}
2\times 0 &2\times 2 \\
2\times 1 &2\times 4
\end{bmatrix} 
=
\begin{bmatrix}
0 &4 \\
2 &8
\end{bmatrix}
$$



在`numpy`中，使用运算符`*`或`np.multiply`计算：

```python
A = np.array([
    [0,2],
    [1,4]
])

>>> 2 * A
array([[0, 4],
       [2, 8]])
>>> np.multiply(2,A)
array([[0, 4],
       [2, 8]])
```



#### 2.2.3矩阵与矩阵相乘（Matrix-matrix multiplication）

矩阵$A \in {\Bbb R}^{m\times n}$与$B \in {\Bbb R}^{n \times p}$相乘，得到新的矩阵$C\in {\Bbb R}^{m\times p}$，新矩阵的


$$
\begin{align*}
A \cdot B &:=
\begin{bmatrix}
&a_{11} & a_{12} &... &a_{1n} \\
&a_{21} & a_{12} &... &a_{2n} \\
&⋮ &⋮  &⋱ &⋮ \\
&a_{m1} & a_{m2} &... &a_{mn} \\

\end{bmatrix}
\begin{bmatrix}
&b_{11} & b_{12} &... &b_{1p} \\
&b_{21} & b_{12} &... &b_{2p} \\
&⋮ &⋮  &⋱ &⋮ \\
&b_{n1} & b_{n2} &... &b_{np} \\

\end{bmatrix}
\\
&=
\begin{bmatrix}
&c_{11} & c_{12} &... &c_{1p} \\
&c_{21} & c_{12} &... &c_{2p} \\
&⋮ &⋮  &⋱ &⋮ \\
&c_{m1} & c_{m2} &... &c_{mp} \\

\end{bmatrix}

\end{align*}
$$



其中$C$具体元素的值定义如下：


$$
c_{ij} :=\sum_{k=1}^n a_{ik}b_{kj},i=1,\dots m, \ j=1,\dots p
$$



如：


$$
A \cdot B = 
\begin{bmatrix}
0 &2  \\
1 &4  \\
3 &1 
\end{bmatrix} 
\begin{bmatrix}
0 &2 \\
1 &4
\end{bmatrix} 
=
\begin{bmatrix}
0 \times 0+2\times 1  &0\times 2 + 2 \times 4 \\
1 \times 0+4\times 1  &1\times 2 + 4 \times 4 \\
3 \times 0+1\times 1  &3\times 2 + 1 \times 4 \\

\end{bmatrix} 
=
\begin{bmatrix}
2 &8 \\
4 &18 \\
1 &10 
\end{bmatrix}
$$



矩阵乘法满足以下性质：

* 结合律（Associativity）：$(AB)C=A(BC)$
* 带标量乘法的结合律：$a(AB)=(aA)B$
* 分配率：$A(B+C)=AB+AC$ 
* 转置乘法：$(AB)^T = B^TA^T$

**需要注意**：$AB\ne BA$

在`numpy`中，使用`@`或`dot`计算：

```python
A = np.array([
    [0,2],
    [1,4],
    [3,1],    
])
B = np.array([
    [0,2],
    [1,4]
])

>>> A @ B
array([[ 2,  8],
       [ 4, 18],
       [ 1, 10]])

>>> np.dot(A,B)
array([[ 2,  8],
       [ 4, 18],
       [ 1, 10]])
```



#### 2.2.4 单位矩阵（identity matrix）

对角线位置为1，其他位置全部为0的正方形矩称为单位矩阵，用$I_n\in {\Bbb R}^{n\times n}$表示：


$$
I_n:=
\begin{bmatrix}
1 & 0 & ⋯ & 0 & 0 \\
0 & 1 & ⋯ & 0 & 0 \\
0 & 0 & ⋱ & 0 & 0 \\
0 & 1 & ⋯ & 1 & 0 \\
0 & 1 & ⋯ & 0 & 1 \\
\end{bmatrix} 
\in
{\Bbb R}^{n\times n}
$$


如：


$$
I_3 =
\begin{bmatrix}
1 &0 &0  \\
0 &1 &0 \\
0 &0 &1
\end{bmatrix}
$$


根据矩阵乘法规则，可以得出$A^{m\times n}\cdot I_n = A^{m \times n}$，即矩阵与单位矩阵相乘等于自身，$I_n$这类似与实数乘法中的$1$。

在`numpy`中，使用`identity`生成单位矩阵：

```python
A = np.array([
    [0,2],
    [1,4],
    [3,1],    
])

I = np.identity(2)

>>> A @ I
array([[0., 2.],
       [1., 4.],
       [3., 1.]])
```

#### 2.2.5 逆矩阵（matrix inverse）

我们知道，实数运算中，$x \times x^{-1} = 1$，类似的，在矩阵中我们定义$A\in {\Bbb R}^{m \times n}$的逆矩阵为$A^{-1}$，它有如下性质：


$$
A^{-1} A=I_n=AA^{-1}
$$


之所以关系逆矩阵，是因为它可以用来解线性方程组，假设一个方程为：


$$
Ax=y
$$


如果$A$存在逆矩阵$A^{-1}$，则可以在方程两边乘以$A^{-1}$，则有：


$$
A^{-1}Ax=A^{-1}y
$$


可以得到：


$$
Ix=A^{-1}y
$$


因为$Ix=x$，最终可以得到：


$$
x=A^{-1}y
$$



需要注意，并不是所有矩阵都是可逆的（线性方程组可能无解）。

在`numpy`中，使用`.linalg.inv`计算：

```python
A = np.array([[1, 2, 1],
              [4, 4, 5],
              [6, 7, 7]])

A_i = np.linalg.inv(A)
>>> A_i
array([[-7., -7.,  6.],
       [ 2.,  1., -1.],
       [ 4.,  5., -4.]])

```

验证一下：

```python
>>> np.round(A @ A_i )
array([[ 1.,  0.,  0.],
       [-0.,  1.,  0.],
       [-0.,  0.,  1.]])
```



#### 2.2.6 矩阵的转置（matrix transpose）

将一个矩阵$A\in {\Bbb R}^{m \times n}$沿着对角线翻转就可以得到转置矩阵$A^T \in {\Bbb R}^{n \times m}$，满足以下关系：


$$
(A^T)_{ij} = A_{ji}
$$


例如：


$$
\begin{bmatrix}
1 &3 &5  \\
2 &4 &6 \\
\end{bmatrix}
^T=
\begin{bmatrix}
1 &2   \\
3 &4  \\
5 &6  \\
\end{bmatrix}
$$



在`numpy`中，使用`T`计算：

```python
A = np.array([[1, 3, 5],
              [2, 4, 6]])

>>> A.T
array([[1, 2],
       [3, 4],
       [5, 6]])
```



#### 2.2.7 哈达玛积（Hadamard product）

需要注意，在上文中提到了矩阵与矩阵的乘法，计算规则如下：


$$
c_{ij} :=\sum_{k=1}^n a_{ik}b_{kj},i=1,\dots m, \ j=1,\dots p
$$


而哈达玛积更简单，它遵循逐个元素操作的原则（element-wise），两个矩阵$A\in {\Bbb R}^{m \times n}$和$B\in {\Bbb R}^{m \times n}$，它们的哈达玛积$C\in {\Bbb R}^{m \times n}=A\odot B$，满足


$$
c_{ij}:= a_{ij} \cdot b_{ij}
$$


例如


$$
A\odot B=
\begin{bmatrix}
1 &2   \\
3 &4  \\
\end{bmatrix}
\begin{bmatrix}
5 &6   \\
7 &8  \\
\end{bmatrix}
=
\begin{bmatrix}
1\times 5 &2\times 6   \\
3\times 7 &4 \times 8  \\
\end{bmatrix}
=
\begin{bmatrix}
5 &12   \\
21 &32  \\
\end{bmatrix}
$$


在`numpy`中，使用`*`或`multiply`计算：

```python
A = np.array([[1, 2],
              [3, 4]])

B = np.array([[5, 6],
              [7, 8]])

>>> A*B
array([[ 5, 12],
       [21, 32]])
>>> np.multiply(A,B)
array([[ 5, 12],
       [21, 32]])
```

### 2.2 用矩阵方法求解线性方程组

下面是一个方程组，我们需要求解$x,y,z$：


$$
\begin{align*}
x-2y+3z = 9 \\
-x+3y =-4\\
2x-5y+5z=17
\end{align*}
$$


转化为线性方程组：


$$
\begin{bmatrix}
1 &-2 &3 \\
-1 &3 &0 \\
2 &-5 &5 
\end{bmatrix}
\begin{bmatrix}
x \\
y \\
z
\end{bmatrix}
=
\begin{bmatrix}
9 \\
-4 \\
17
\end{bmatrix}
$$


转为增广矩阵（augmented matrix）：


$$
\begin{bmatrix}

\begin{array}{ccc|c}
1 &-2 &3 &9\\
-1 &3 &0 &-4\\
2 &-5 &5 &17

\end{array}
    
\end{bmatrix}
$$

#### 2.2.1 高斯消元（Gaussian Elimination）

高斯消元法（gaussian elimination）是求解线性方程组的一种方法，他的目标是将线性方程组的增广矩阵转化为行阶梯矩阵（Row-Echelon Form）的形式求解：


$$
\begin{bmatrix}

\begin{array}{ccc|c}
1 &a &b &d \\
0 &1 &c &e \\
0 &0 &1 &f \\
\end{array}
    
\end{bmatrix}
$$


**基本的行操作方法：**

1. 交换两行：记为$R_i\leftrightarrow R_j$
2. 将行乘以非0数：记为$R_i \rightarrow NR_j$
3. 将两行相加赋予另一列：记为$R_i+R_j \rightarrow R_j$

**具体操作步骤**：

<img src="https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210715174405.png?x-oss-process=style/wp" style="zoom:50%;" />

1. 使第1行，第1列变成1
2. 用1将第1列的其他位置变成0
3. 使第2行，第2列变成1
4. 用1将第2列的其他位置变成0
5. 使第3行，第3列变成1
6. 以此类推

例如，求解上文说的方程组：


$$
\begin{align*}
x-2y+3z = 9 \\
-x+3y =-4\\
2x-5y+5z=17
\end{align*}
$$


具体步骤如下：


$$
\begin{align*}
\begin{bmatrix}

\begin{array}{ccc|c}
1 &-2 &3 &9\\
-1 &3 &0 &-4\\
2 &-5 &5 &17

\end{array}
    
\end{bmatrix}

\ \ 
R_1+R_2 \rightarrow R_2

\begin{bmatrix}

\begin{array}{ccc|c}
1 &-2 &3 &9\\
0 &1 &3 &5\\
2 &-5 &5 &17

\end{array}
    
\end{bmatrix}
\\ 
-2R_1+R_3 \rightarrow R_3

\begin{bmatrix}

\begin{array}{ccc|c}
1 &-2 &3 &9\\
0 &1 &3 &5\\
0 &-1 &-1 &-1

\end{array}
    
\end{bmatrix}

\\
R_2+R_3 \rightarrow R_3

\begin{bmatrix}

\begin{array}{ccc|c}
1 &-2 &3 &9\\
0 &1 &3 &5\\
0 &0 &2 &4

\end{array}
    
\end{bmatrix}

\\  
\frac{1}{2}R_3 \rightarrow R_3

\begin{bmatrix}

\begin{array}{ccc|c}
1 &-2 &3 &9\\
0 &1 &3 &5\\
0 &0 &1 &2

\end{array}
    
\end{bmatrix}
\end{align*}
$$


我们可以得到一个新的方程组：


$$
\begin{align*}
x-2y+3z = 9 \\
y + 3z =5 \\
z=2
\end{align*}
$$


所以：


$$
\begin{align*}
x=1 \\
y=-1 \\
z=2
\end{align*}
$$


在`numpy`中用`linalg.solve`求解：

```python
import numpy as np
A = np.array([
    [1,-2,3],
    [-1,3,0],
    [2,-5,5]    
])
y = np.array([
    [9],
    [-4],
    [17]
])

>>> np.linalg.solve(A,y)
array([[ 1.],
       [-1.],
       [ 2.]])
```



#### 2.2.2 高斯-约旦消元

基于高斯消元法可以得到：


$$
\begin{bmatrix}

\begin{array}{ccc|c}
1 &a &b &d \\
0 &1 &c &e \\
0 &0 &1 &f \\
\end{array}
    
\end{bmatrix}
$$


再前进一步，将它修改为：


$$
\begin{bmatrix}

\begin{array}{ccc|c}
1 &0 &0 &a \\
0 &1 &0 &b \\
0 &0 &1 &c \\
\end{array}
    
\end{bmatrix}
$$


我们就可以直接得到：


$$
\begin{align*}
x = a \\
y = b \\
z = c
\end{align*}
$$



这便是高斯-约旦消元法，具体步骤可参考下图：

<img src="https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210715213550.png?x-oss-process=style/wp" style="zoom:50%;" />



### 2.3 矩阵范数（matrix norm）

#### 2.3.1 F-范数（Frobenius norm）

对于矩阵$A^{m \times n}$，它的F-范数$||A||_F$定义如下：
$$
||A||_F :=\sqrt{\sum_{i=1}^m \sum_{j=1}^n a_{ij}^2}
$$


如：
$$
\begin{align}
A = 
\begin{bmatrix}
1 &2 &3 \\
4 &5 &6 \\
7 &8 &9
\end{bmatrix}
, 

||A||_F &= \sqrt{1^2+2^2+3^2+4^2+5^2+6^2+7^2+8^2+9^2} \\
&=16.881943016134134
\end{align}
$$


在`numpy`中，使用`np.linalg.norm`计算：

```python
A = np.array([[1, 2, 3],
              [4, 5, 6], 
              [7, 8, 9]])
>>> np.linalg.norm(A,'fro')
16.881943016134134
```



#### 2.3.2 最大值范数（Max norm）

对于矩阵$A^{m \times n}$，它的最大值范数$||A||_{ax}$定义如下：
$$
||A||_{ax} :=max_i \sum_{j=1}^n |a_{ij}|
$$
算法如下：

1. 计算每行元素绝对值的和；
2. 找出最大值得和作为范数。



如：
$$
A = 
\begin{bmatrix}
1 &2 &3 \\
-4 &5 &6 \\
7 &-8 &9
\end{bmatrix}
$$


的最大值范数$7 + |-8|+9 = 24$。

在`numpy`中，使用`np.linalg.norm`计算：

```python
A = np.array([[1, 2, 3],
              [-4, 5, 6], 
              [7, -8, 9]])

>>> np.linalg.norm(A,np.inf)
24.0
```



#### 2.3.3 谱范数（spectral norm）

待完善。



## 三、线性映射与仿射映射（linear and affine mapping）

### 3.1 线性映射

假设一个线性映射函数`T`和一组向量`x,y`，需要满足以下条件：
$$
T(x+y) = T(x) + T(y) \\
T(\alpha x) = aT(x),\forall \alpha
$$

即：

* 分配率：对一组向量和的转化效果要等于分别对这两个向量处理结果的和；
* 结合律

这两个性质可以统一成一个叠加原理（superposition property）：
$$
T(\alpha x + \alpha y) = \alpha T(x) + \alpha T(y)
$$
在线性代数中，线性映射可以用矩阵乘法来表达：
$$
T(x) = Ax
$$

### 3.2 仿射映射

待完善。

### 3.3 各种特殊的线性映射

#### 3.3.1 缩放（scaling）

使用$y=Ax,A=\alpha I$进行缩放，其中：

* $\alpha >1$时，将$x$拉伸放大
* $\alpha <1$时，将$x$压缩缩小
* $\alpha < 0$时，倒置矢量$\vec x$

如一个大小为${\Bbb R}^2$的缩放矩阵：
$$
\begin{bmatrix}
s_1 &0 \\
0 &s_2
\end{bmatrix}
$$
其中$s_1,s_2$为缩放因子（scaling factors），如：
$$
x  = 
\begin{bmatrix}
0 & 2 \\
1 & 4
\end{bmatrix}
,
A =
\begin{bmatrix}
3 & 0 \\
0 & 3
\end{bmatrix}
$$
则缩放后为：
$$
y = Ax = 
\begin{bmatrix}
0 & 6 \\
3 & 12
\end{bmatrix}
$$
在`numpy`中验证：

```python
import numpy as np
x = np.array([
    [0,2],
    [1,4]
])
A = np.array([
    [3,0],
    [0,3]
])
>>> A @ x
array([[ 0,  6],
       [ 3, 12]])
```

#### 3.3.2 镜像（reflection）

如下图，在笛卡尔坐标系中，一条穿过坐标原点的线（图中绿线）与横坐标夹角为$\theta$，矢量$\vec a$ 相对于绿线的镜像为$\vec a'$

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210722115529.png?x-oss-process=style/wp)

则有


$$
\vec a' = 
\begin{bmatrix}
cos(2\theta) &sin(2\theta) \\
sin(2\theta) &-cos(2\theta)
\end{bmatrix}
\vec a
$$


1. 以横坐标$x$做镜像：
$$
\vec a' = 
 \begin{bmatrix}
 1 &0 \\
 0 &-1
 \end{bmatrix}
 \vec a
$$


2. 以纵坐标$y$做镜像：

$$
\vec a' = 
\begin{bmatrix}
-1 &0 \\
0 &1
\end{bmatrix}
\vec a
$$

3. 沿着$y=x$的直线（$\theta=45^{\circ}$）做镜像：

$$
\vec a' = 
\begin{bmatrix}
0 &1 \\
1 &0
\end{bmatrix}
\vec a
$$

4. 沿着$y=-x$的直线（$\theta=-45^{\circ}$）做镜像：

$$
\vec a' = 
\begin{bmatrix}
0 &-1 \\
-1 &0
\end{bmatrix}
\vec a
$$

如$\vec a=[0,2]^T$，则分别有：
$$
\begin{align}

&1.横坐标镜像：
\vec a' =
 \begin{bmatrix}
 1 &0 \\
 0 &-1
 \end{bmatrix}
\begin{bmatrix}
0 \\
2
\end{bmatrix}
=
\begin{bmatrix}
0 \\
-2
\end{bmatrix}
\\ \\
&2.纵坐标镜像：
\vec a' =
 \begin{bmatrix}
 -1 &0 \\
 0 &0
 \end{bmatrix}
\begin{bmatrix}
0 \\
2
\end{bmatrix}
=
\begin{bmatrix}
0 \\
2
\end{bmatrix}
\\ \\
&3.\theta=45^{\circ}镜像：
\vec a' =
 \begin{bmatrix}
0 &1 \\
1 &0
 \end{bmatrix}
\begin{bmatrix}
0 \\
2
\end{bmatrix}
=
\begin{bmatrix}
2 \\
0
\end{bmatrix}
\\ \\
&4.\theta=-45^{\circ}镜像：
\vec a' =
 \begin{bmatrix}
0 &-1 \\
-1 &0
 \end{bmatrix}
\begin{bmatrix}
0 \\
2
\end{bmatrix}
=
\begin{bmatrix}
-2 \\
0
\end{bmatrix}
\end{align}
$$
在`numpy`中验证：

```python
# rotation along the horiontal axis
A1 = np.array([[1.0, 0],
               [0, -1.0]])

# rotation along the vertical axis
A2 = np.array([[-1.0, 0],
               [0, 1.0]])

# rotation along the line at 45 degrees from the origin
A3 = np.array([[0, 1.0],
               [1.0, 0]])

# rotation along the line at -45 degrees from the origin
A4 = np.array([[0, -1.0],
               [-1.0, 0]])

x = np.array([
    [0],
    [2]
])

>>> A1 @ x
array([[ 0.],
       [-2.]])

>>> A2 @ x
array([[0.],
       [2.]])

>>> A3 @ x
array([[2.],
       [0.]])

>>> A4 @ x
array([[-2.],
       [ 0.]])
```

#### 3.3.3 错切（shear）

如图，错切可以简单理解成将矩形变为平行四边形的过程。

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210722151945.png?x-oss-process=style/wp)

在笛卡尔坐标系中，矢量$\vec a\in{\Bbb R}^2$ 沿着横坐标错切后的$\vec a'$为：


$$
\vec a' = 
\begin{bmatrix}
1 &m \\
0 &1
\end{bmatrix}
\vec a
$$
其中$m$是错切因子（shear factor），它决定了错切的程度，如上图图分别是$m=1$和$m=2$的错切后效果图（想象一下，把图片上每个点都做错切）。



矢量$\vec a\in{\Bbb R}^2$ 沿着纵坐标坐标错切后的$\vec a'$为：
$$
\vec a' = 
 \begin{bmatrix}
 1 &0 \\
 m &1
 \end{bmatrix}
 \vec a
$$
下图分别是$m=1$和$m=2$的错切后效果图

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210722152241.png?x-oss-process=style/wp)

#### 3.3.3 旋转（rotation）

在笛卡尔坐标系中，矢量$\vec a\in{\Bbb R}^2$ 围绕坐标原点逆时针旋转$\theta$后的$\vec a'$为：
$$
\vec a' = 
\begin{bmatrix}
cos\theta &-sin\theta \\
sin\theta &cos\theta
\end{bmatrix}
\vec a
$$


如逆时针旋转$90^\circ$：
$$
\vec a' = 
\begin{bmatrix}
0 &-1 \\
1 &0

\end{bmatrix}
\vec a
$$
效果图如下：

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210722160716.png?x-oss-process=style/wp)

### 3.4 投射（projections）

