---
layout: post
title: '机器学习相关线性代数简介（持续完善中）'
date: 2021-07-12
tags:
  线性代数
  linear-algebra
  data-science
  vector
  matrix
---
> 参考资料：
>
> * [mathematics for machine learning](https://mml-book.github.io/)
> * [Introduction to Linear Algebra for Applied Machine Learning with Python](https://pabloinsente.github.io/intro-linear-algebra)
>* [Matrices: Gaussian & Gauss-Jordan Elimination](https://www.craftonhills.edu/current-students/tutoring-center/mathematics-tutoring/matrices-gauss-jordan.pdf)
> * 起始编辑时间：2021-07-12

[toc]

## 一、向量（vectors）

### 1.1类型

#### 1.1.1 几何向量：

我们高中时学到、见到的最多的类型，一般在二维或三维中作图就可以表示

<img src="https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210712154844.png?x-oss-process=style/wp" style="zoom:50%;" />



#### 1.1.2 多项式：

多项式如$f(x)=x^2+y+1$，之所以也称为向量是因为它满足向量的定义：

* 可以用加法，得到新的多项式；
* 可以用乘法，得到新的多项式；

如$f(x)+g(x)$和$5\times f(x)$

<img src="https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210712155230.png?x-oss-process=style/wp" style="zoom:50%;" />



#### 1.1.3 实数集合：

广义地说，任意实数组成的集合 ${\Bbb R}^n$ 也是向量，这是机器学习中最常见、最重要的向量：



$$
X = 
\begin{bmatrix}
x_1 \\
x_2 \\
. \\
. \\
. \\
x_n
\end{bmatrix}
\in 
{\Bbb R}^n
$$


如，一个$  {\Bbb R}^3$ 3维的向量：


$$
x = 
\begin{bmatrix}
x_1 \\
x_2 \\
x_3 \\
\end{bmatrix}
\in
{\Bbb R}^3
$$



### 1.2向量的基本运算

#### 1.2.1 加法

两个大小一样的向量的加法，需要向量的每个元素逐个相加（element-wise）：


$$
x+y =
\begin{bmatrix}
x_1 \\
. \\
. \\
. \\
x_n
\end{bmatrix}
+
\begin{bmatrix}
y_1 \\
. \\
. \\
. \\
y_n
\end{bmatrix}
=
\begin{bmatrix}
x_1 + y_1 \\
. \\
. \\
. \\
x_n + y_n
\end{bmatrix}
$$


如


$$
x+y = 
\begin{bmatrix}
1 \\
2 \\
3 \\
\end{bmatrix}
+
\begin{bmatrix}
3 \\
4 \\
5 \\
\end{bmatrix}
=
\begin{bmatrix}
1 + 3 \\
2 + 4 \\
3 + 5
\end{bmatrix}
=
\begin{bmatrix}
4 \\
6 \\
8 
\end{bmatrix}
$$


向量加法满足以下性质：

* 交换律（Commutativity）：$x + y = y + x$
* 结合律（Associativity）：$(x+y)+z=x+(y+z)$
* 零向量$\vec{0}$无效：$x+0 = x$
* 减去自身等于零向量$\vec{0}$：$x-x = \vec{0}$

在`numpy`中，使用运算符`+`或`add`方法计算两个向量的和

```python
import numpy as np
# 赋值x,y为向量[1,2,3]
x = y = np.array([[1],
                  [2],
                  [3]])
>>> x + y
array([[2],
       [4],
       [6]])

>>> np.add(x,y)
array([[2],
       [4],
       [6]])
```

#### 1.2.2向量与标量相乘（vector-scalar multiplication）

向量与标量的乘法也遵循逐个元素操作的原则（element-wise）：


$$
ax = 
\begin{bmatrix}
ax_1 \\
. \\
. \\
. \\
ax_n \\
\end{bmatrix}
$$



假设$a=3$，$x=[1,2,3]^T$，则


$$
ax = 3
\begin{bmatrix}
1 \\
2 \\
3 \\
\end{bmatrix}
=
\begin{bmatrix}
3\times1 \\
3\times2 \\
3\times3 \\
\end{bmatrix}
=
\begin{bmatrix}
3 \\
6 \\
9 \\
\end{bmatrix}
$$


向量与标量的乘法满足以下性质：

* 结合律（Associativity）：$(\alpha\beta)x=\alpha(\beta x)$
* 左分配率（Left-distributive）：$(\alpha+\beta)x=\alpha x+ \beta x$
* 右分配率（Right-distributive）：$x(\alpha+\beta)=x\alpha + x\beta $
* 其他：$\alpha(x+y)=\alpha x + \alpha y$

在`numpy`中，使用运算符`*`或`np.multiply`计算向量与标量乘积

```python
import numpy as np

x = 3
y = np.array([
    [3],
    [4],
    [5],
    [6]
])

>>> x*y
array([[ 9],
       [12],
       [15],
       [18]])
```

#### 1.2.3向量的线性组合

$$
\alpha x  + \beta y = \alpha 
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix}
+
\beta
\begin{bmatrix}
y_1 \\
y_2
\end{bmatrix}
=
\begin{bmatrix}
\alpha x_1+ \beta y_1 \\
\alpha x_2 + \beta y_2
\end{bmatrix}
$$



如 $\alpha =2 , \beta=3,x=[2,3]^T,y=[4,5]^T$，则


$$
\alpha x + \beta y = 
2 
\begin{bmatrix}
2 \\
3
\end{bmatrix}
+
3
\begin{bmatrix}
4 \\
5
\end{bmatrix}
=
\begin{bmatrix}
2 \times 2 + 3 \times 4 \\
2 \times 3 + 3 \times 5
\end{bmatrix}
=
\begin{bmatrix}
16 \\
21
\end{bmatrix}
$$


线性组合的另一种方式是用求和公式，假设$x_i,...x_k$是一个向量，标量集合$\beta_i,...,\beta_k\in {\Bbb R}$，则


$$
\sum_{i=1}^k \beta_i x_i := \beta_1x_1 + ...+\beta_kx_k
$$


公式中$:=$表示“定义为”的意思。

在`numpy`中，计算向量的线性组合

```python
import numpy as np
a,b = 2,3
x,y = np.array([[2],[3]]), np.array([[4],[5]])

>>> a * x + b * y
array([[16],
       [21]])
```



### 1.3向量空间

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210712212634.png?x-oss-process=style/wp)

### 1.4向量子空间



### 1.5 向量范数（vector norms）

在机器学习算法中，经常使用到向量的范数，范数可以简单地理解为向量的长度（从原点到终点），如图，一般常用的范数有以下几种：

![](https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210713165010.png?x-oss-process=style/wp)



#### 1.5.1 欧几里得范数（Euclidean norm）

欧几里得范数$L_2$最常用，它的定义为：


$$
||x||_2 := \sqrt{\sum_{i=1}^n x_i^2}
$$


如，二维向量的$L_2$范数为：


$$
||x||_2 \in {\rm I\!R} ^2 = \sqrt{x_1^2+x_2^2}
$$


在`numpy`中，用`np.linalg.norm`计算范数：

```python
import numpy as np
a = np.array([[3],[4]])
>>> np.linalg.norm(a,2)
5.0
```

上图第一个坐标图表达的是$L_2$范数为1的二维向量形成的区域，可以看到它是一个圆，之所以是一个圆是因为：



假设向量$a = [x,y]$是一个二维向量，根据$L_2$范数定义：


$$
||a||_2=\sqrt{x^2+y^2}
$$


正好$x,y$分别是向量在坐标系中的横坐标和纵坐标，所以$L_2$长度正好是三角形的斜边。



<img src="https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210713172736.png?x-oss-process=style/wp" style="zoom:50%;" />



#### 1.5.2 曼哈顿范数（Manhattan norm）

曼哈顿范数$L_1$，定义如下：


$$
||x||_1 :=\sum_{i=1}^n |x_i|
$$



如，二维向量的$L_1$范数为：


$$
||x||_1 \in {\rm I\!R} ^2 = |x_1|+|x_2|
$$


在`numpy`中，用`np.linalg.norm`计算范数：

```
import numpy as np
a = np.array([[3],[-4]])
>>> np.linalg.norm(a,1)
7.0
```

如下图，是$L_1$范数为1的二维向量形成的区域，可以看到它是一个方形（之所以叫曼哈顿范数，是因为$L_1$类似曼哈顿的网格型地形）。

<img src="https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210713180519.png?x-oss-process=style/wp" style="zoom: 50%;" />

如下图，之所以$L_1$之所以是一个方形是因为：

假设向量$a = [x,y]$是一个二维向量，根据$L_1$范数定义


$$
||a||_1=|x|+|y|
$$


正好$x,y$分别是向量在坐标系中的横坐标和纵坐标，所以$L_1$长度正好是三角形的两个直角边的长度和，又因为$y=z$（等腰直角三角形），所以


$$
L_1=|x|+|y|=|x|+|z|=1
$$


<img src="https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210713181531.png?x-oss-process=style/wp" style="zoom:50%;" />



#### 1.5.3 最大值范数（Max norm）

最大值范数很简单，就是取向量元素绝对值最大的值即可：


$$
||x||_{\infty} :=  max_i|x_i|
$$

例如，假设向量$x = [3,-4,-5]$的
$$
||x||_\infty=5
$$


在`numpy`中，用`np.linalg.norm`计算范数：

```python
import numpy as np
a = np.array([[3],[-4],[-5]])
>>> np.linalg.norm(a,np.inf)
5.0
```



### 1.6 线性方程组（systems  of linear equations）

一般线性方程组如下：


$$
\begin{align*}
&x_1 &+ x_2 &+x_3 = 3 \\
&x_1 &- x_2 &+ 2x_3 = 2 \\
&2x_1 & &+ 3x_3=5 
\end{align*}
$$


将方程组系数$a_{ij}$挑出，组成向量（vectors）形式：


$$
\begin{bmatrix}
a_{11}\\
.\\
.\\
.\\
a_{m1}
\end{bmatrix}
x_1 
+ 
\begin{bmatrix}
a_{12}\\
.\\
.\\
.\\
a_{m2}
\end{bmatrix}
x_2

+ ... + 
\begin{bmatrix}
a_{1n}\\
.\\
.\\
.\\
a_{mn}
\end{bmatrix}
x_n
=
\begin{bmatrix}
b_1\\
.\\
.\\
.\\
b_m
\end{bmatrix}
$$



再将向量变成矩阵（matrices）形式：


$$
\begin{bmatrix}
&a_{11} &...&a_{1n}\\
&. & &. \\
&. & &. \\
&. & &. \\
&a_{m1} &...&a_{mn}
\end{bmatrix}
\begin{bmatrix}
x_1 \\
. \\
. \\
. \\
x_n
\end{bmatrix}
=
\begin{bmatrix}
b_1 \\
. \\
. \\
. \\
b_n
\end{bmatrix}
$$



## 二、矩阵（matrices）

一个有着$m$行$n$列的矩阵$A\in {\Bbb R}^{m \times n}$的定义如下：


$$
A :=
\begin{bmatrix}
&a_{11} & a_{12} &... &a_{1n} \\
&a_{21} & a_{12} &... &a_{2n} \\
&⋮ &⋮  &⋱ &⋮ \\
&a_{m1} & a_{m2} &... &a_{mn} \\

\end{bmatrix}
,
a_{ij} \in {\rm I\!R}
$$


在`numpy`中，构造一个矩阵：

```python
import numpy as np
A = np.array([
    [1,7],
    [3,5]
])

>>> A
array([[1, 7],
       [3, 5]])
```



### 2.1 基本运算

#### 2.1.1 加法

矩阵加法遵循逐个元素操作的原则（element-wise），矩阵$A\in{\Bbb R}^{m\times n}$和$B\in {\Bbb R}^{m\times n}$的和为：


$$
A+B :=
\begin{bmatrix}
&a_{11}+b_{11} & a_{12} + b_{12} &... &a_{1n} + b_{1n}\\
&a_{21}+b_{21} & a_{22} + b_{22} &... &a_{2n} + b_{2n}\\
&⋮ &⋮  &⋱ &⋮ \\
&a_{m1} +b_{m1} & a_{m2}+ b_{m2} &... &a_{mn} +b_{mn}\\

\end{bmatrix}
\in
{\rm I\!R}^{m\times n}
$$



例如：


$$
A=
\begin{bmatrix}
0 &2 \\
1 &4
\end{bmatrix},
B=
\begin{bmatrix}
3 &1 \\
-3 &2
\end{bmatrix}
,
则 \ A+B=
\begin{bmatrix}
3+0 &2+1 \\
1-3 &4+2
\end{bmatrix}
=
\begin{bmatrix}
3 &3 \\
-2 &6
\end{bmatrix}
$$



在`numpy`中，类似向量的操作，用`+`或`np.add`运算：

```python
A = np.array([
    [0,2],
    [1,4]
])
B = np.array([
    [3,1],
    [-3,2]
])

>>> A+B
array([[ 3,  3],
       [-2,  6]])
>>> np.add(A,B)
array([[ 3,  3],
       [-2,  6]])
```



#### 2.2.2矩阵与标量相乘（Matrix-scalar multiplication）

矩阵与标量的乘法也遵循逐逐个元素操作的原则（element-wise）：矩阵$A$与标量$a$相乘，使矩阵每个元素$a_{ij}$都乘以$a$，即$a_{ij}\times a$

如：


$$
2 \times 
\begin{bmatrix}
0 &2 \\
1 &4
\end{bmatrix} 
=
\begin{bmatrix}
2\times 0 &2\times 2 \\
2\times 1 &2\times 4
\end{bmatrix} 
=
\begin{bmatrix}
0 &4 \\
2 &8
\end{bmatrix}
$$



在`numpy`中，使用运算符`*`或`np.multiply`计算：

```python
A = np.array([
    [0,2],
    [1,4]
])

>>> 2 * A
array([[0, 4],
       [2, 8]])
>>> np.multiply(2,A)
array([[0, 4],
       [2, 8]])
```



#### 2.2.3矩阵与矩阵相乘（Matrix-matrix multiplication）

矩阵$A \in {\Bbb R}^{m\times n}$与$B \in {\Bbb R}^{n \times p}$相乘，得到新的矩阵$C\in {\Bbb R}^{m\times p}$，新矩阵的


$$
\begin{align*}
A \cdot B &:=
\begin{bmatrix}
&a_{11} & a_{12} &... &a_{1n} \\
&a_{21} & a_{12} &... &a_{2n} \\
&⋮ &⋮  &⋱ &⋮ \\
&a_{m1} & a_{m2} &... &a_{mn} \\

\end{bmatrix}
\begin{bmatrix}
&b_{11} & b_{12} &... &b_{1p} \\
&b_{21} & b_{12} &... &b_{2p} \\
&⋮ &⋮  &⋱ &⋮ \\
&b_{n1} & b_{n2} &... &b_{np} \\

\end{bmatrix}
\\
&=
\begin{bmatrix}
&c_{11} & c_{12} &... &c_{1p} \\
&c_{21} & c_{12} &... &c_{2p} \\
&⋮ &⋮  &⋱ &⋮ \\
&c_{m1} & c_{m2} &... &c_{mp} \\

\end{bmatrix}

\end{align*}
$$



其中$C$具体元素的值定义如下：


$$
c_{ij} :=\sum_{k=1}^n a_{ik}b_{kj},i=1,\dots m, \ j=1,\dots p
$$



如：


$$
A \cdot B = 
\begin{bmatrix}
0 &2  \\
1 &4  \\
3 &1 
\end{bmatrix} 
\begin{bmatrix}
0 &2 \\
1 &4
\end{bmatrix} 
=
\begin{bmatrix}
0 \times 0+2\times 1  &0\times 2 + 2 \times 4 \\
1 \times 0+4\times 1  &1\times 2 + 4 \times 4 \\
3 \times 0+1\times 1  &3\times 2 + 1 \times 4 \\

\end{bmatrix} 
=
\begin{bmatrix}
2 &8 \\
4 &18 \\
1 &10 
\end{bmatrix}
$$



矩阵乘法满足以下性质：

* 结合律（Associativity）：$(AB)C=A(BC)$
* 带标量乘法的结合律：$a(AB)=(aA)B$
* 分配率：$A(B+C)=AB+AC$ 
* 转置乘法：$(AB)^T = B^TA^T$

**需要注意**：$AB\ne BA$

在`numpy`中，使用`@`或`dot`计算：

```python
A = np.array([
    [0,2],
    [1,4],
    [3,1],    
])
B = np.array([
    [0,2],
    [1,4]
])

>>> A @ B
array([[ 2,  8],
       [ 4, 18],
       [ 1, 10]])

>>> np.dot(A,B)
array([[ 2,  8],
       [ 4, 18],
       [ 1, 10]])
```



#### 2.2.4 单位矩阵（identity matrix）

对角线位置为1，其他位置全部为0的正方形矩称为单位矩阵，用$I_n\in {\Bbb R}^{n\times n}$表示：


$$
I_n:=
\begin{bmatrix}
1 & 0 & ⋯ & 0 & 0 \\
0 & 1 & ⋯ & 0 & 0 \\
0 & 0 & ⋱ & 0 & 0 \\
0 & 1 & ⋯ & 1 & 0 \\
0 & 1 & ⋯ & 0 & 1 \\
\end{bmatrix} 
\in
{\Bbb R}^{n\times n}
$$


如：


$$
I_3 =
\begin{bmatrix}
1 &0 &0  \\
0 &1 &0 \\
0 &0 &1
\end{bmatrix}
$$


根据矩阵乘法规则，可以得出$A^{m\times n}\cdot I_n = A^{m \times n}$，即矩阵与单位矩阵相乘等于自身，$I_n$这类似与实数乘法中的$1$。

在`numpy`中，使用`identity`生成单位矩阵：

```python
A = np.array([
    [0,2],
    [1,4],
    [3,1],    
])

I = np.identity(2)

>>> A @ I
array([[0., 2.],
       [1., 4.],
       [3., 1.]])
```

#### 2.2.5 逆矩阵（matrix inverse）

我们知道，实数运算中，$x \times x^{-1} = 1$，类似的，在矩阵中我们定义$A\in {\Bbb R}^{m \times n}$的逆矩阵为$A^{-1}$，它有如下性质：


$$
A^{-1} A=I_n=AA^{-1}
$$


之所以关系逆矩阵，是因为它可以用来解线性方程组，假设一个方程为：


$$
Ax=y
$$


如果$A$存在逆矩阵$A^{-1}$，则可以在方程两边乘以$A^{-1}$，则有：


$$
A^{-1}Ax=A^{-1}y
$$


可以得到：


$$
Ix=A^{-1}y
$$


因为$Ix=x$，最终可以得到：


$$
x=A^{-1}y
$$



需要注意，并不是所有矩阵都是可逆的（线性方程组可能无解）。

在`numpy`中，使用`.linalg.inv`计算：

```python
A = np.array([[1, 2, 1],
              [4, 4, 5],
              [6, 7, 7]])

A_i = np.linalg.inv(A)
>>> A_i
array([[-7., -7.,  6.],
       [ 2.,  1., -1.],
       [ 4.,  5., -4.]])

```

验证一下：

```python
>>> np.round(A @ A_i )
array([[ 1.,  0.,  0.],
       [-0.,  1.,  0.],
       [-0.,  0.,  1.]])
```



#### 2.2.6 矩阵的转置（matrix transpose）

将一个矩阵$A\in {\Bbb R}^{m \times n}$沿着对角线翻转就可以得到转置矩阵$A^T \in {\Bbb R}^{n \times m}$，满足以下关系：


$$
(A^T)_{ij} = A_{ji}
$$


例如：


$$
\begin{bmatrix}
1 &3 &5  \\
2 &4 &6 \\
\end{bmatrix}
^T=
\begin{bmatrix}
1 &2   \\
3 &4  \\
5 &6  \\
\end{bmatrix}
$$



在`numpy`中，使用`T`计算：

```python
A = np.array([[1, 3, 5],
              [2, 4, 6]])

>>> A.T
array([[1, 2],
       [3, 4],
       [5, 6]])
```



#### 2.2.7 哈达玛积（Hadamard product）

需要注意，在上文中提到了矩阵与矩阵的乘法，计算规则如下：


$$
c_{ij} :=\sum_{k=1}^n a_{ik}b_{kj},i=1,\dots m, \ j=1,\dots p
$$


而哈达玛积更简单，它遵循逐个元素操作的原则（element-wise），两个矩阵$A\in {\Bbb R}^{m \times n}$和$B\in {\Bbb R}^{m \times n}$，它们的哈达玛积$C\in {\Bbb R}^{m \times n}=A\odot B$，满足


$$
c_{ij}:= a_{ij} \cdot b_{ij}
$$


例如


$$
A\odot B=
\begin{bmatrix}
1 &2   \\
3 &4  \\
\end{bmatrix}
\begin{bmatrix}
5 &6   \\
7 &8  \\
\end{bmatrix}
=
\begin{bmatrix}
1\times 5 &2\times 6   \\
3\times 7 &4 \times 8  \\
\end{bmatrix}
=
\begin{bmatrix}
5 &12   \\
21 &32  \\
\end{bmatrix}
$$


在`numpy`中，使用`*`或`multiply`计算：

```python
A = np.array([[1, 2],
              [3, 4]])

B = np.array([[5, 6],
              [7, 8]])

>>> A*B
array([[ 5, 12],
       [21, 32]])
>>> np.multiply(A,B)
array([[ 5, 12],
       [21, 32]])
```

### 2.2 用矩阵方法求解线性方程组

下面是一个方程组，我们需要求解$x,y,z$：


$$
\begin{align*}
x-2y+3z = 9 \\
-x+3y =-4\\
2x-5y+5z=17
\end{align*}
$$


转化为线性方程组：


$$
\begin{bmatrix}
1 &-2 &3 \\
-1 &3 &0 \\
2 &-5 &5 
\end{bmatrix}
\begin{bmatrix}
x \\
y \\
z
\end{bmatrix}
=
\begin{bmatrix}
9 \\
-4 \\
17
\end{bmatrix}
$$


转为增广矩阵（augmented matrix）：


$$
\begin{bmatrix}

\begin{array}{ccc|c}
1 &-2 &3 &9\\
-1 &3 &0 &-4\\
2 &-5 &5 &17

\end{array}
    
\end{bmatrix}
$$

#### 2.2.1 高斯消元（Gaussian Elimination）

高斯消元法（gaussian elimination）是求解线性方程组的一种方法，他的目标是将线性方程组的增广矩阵转化为行阶梯矩阵（Row-Echelon Form）的形式求解：


$$
\begin{bmatrix}

\begin{array}{ccc|c}
1 &a &b &d \\
0 &1 &c &e \\
0 &0 &1 &f \\
\end{array}
    
\end{bmatrix}
$$


**基本的行操作方法：**

1. 交换两行：记为$R_i\leftrightarrow R_j$
2. 将行乘以非0数：记为$R_i \rightarrow NR_j$
3. 将两行相加赋予另一列：记为$R_i+R_j \rightarrow R_j$

**具体操作步骤**：

<img src="https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210715174405.png?x-oss-process=style/wp" style="zoom:50%;" />

1. 使第1行，第1列变成1
2. 用1将第1列的其他位置变成0
3. 使第2行，第2列变成1
4. 用1将第2列的其他位置变成0
5. 使第3行，第3列变成1
6. 以此类推

例如，求解上文说的方程组：


$$
\begin{align*}
x-2y+3z = 9 \\
-x+3y =-4\\
2x-5y+5z=17
\end{align*}
$$


具体步骤如下：


$$
\begin{align*}
\begin{bmatrix}

\begin{array}{ccc|c}
1 &-2 &3 &9\\
-1 &3 &0 &-4\\
2 &-5 &5 &17

\end{array}
    
\end{bmatrix}

\ \ 
R_1+R_2 \rightarrow R_2

\begin{bmatrix}

\begin{array}{ccc|c}
1 &-2 &3 &9\\
0 &1 &3 &5\\
2 &-5 &5 &17

\end{array}
    
\end{bmatrix}
\\ 
-2R_1+R_3 \rightarrow R_3

\begin{bmatrix}

\begin{array}{ccc|c}
1 &-2 &3 &9\\
0 &1 &3 &5\\
0 &-1 &-1 &-1

\end{array}
    
\end{bmatrix}

\\
R_2+R_3 \rightarrow R_3

\begin{bmatrix}

\begin{array}{ccc|c}
1 &-2 &3 &9\\
0 &1 &3 &5\\
0 &0 &2 &4

\end{array}
    
\end{bmatrix}

\\  
\frac{1}{2}R_3 \rightarrow R_3

\begin{bmatrix}

\begin{array}{ccc|c}
1 &-2 &3 &9\\
0 &1 &3 &5\\
0 &0 &1 &2

\end{array}
    
\end{bmatrix}
\end{align*}
$$


我们可以得到一个新的方程组：


$$
\begin{align*}
x-2y+3z = 9 \\
y + 3z =5 \\
z=2
\end{align*}
$$


所以：


$$
\begin{align*}
x=1 \\
y=-1 \\
z=2
\end{align*}
$$


在`numpy`中用`linalg.solve`求解：

```python
import numpy as np
A = np.array([
    [1,-2,3],
    [-1,3,0],
    [2,-5,5]    
])
y = np.array([
    [9],
    [-4],
    [17]
])

>>> np.linalg.solve(A,y)
array([[ 1.],
       [-1.],
       [ 2.]])
```



#### 2.2.2 高斯-约旦消元

基于高斯消元法可以得到：


$$
\begin{bmatrix}

\begin{array}{ccc|c}
1 &a &b &d \\
0 &1 &c &e \\
0 &0 &1 &f \\
\end{array}
    
\end{bmatrix}
$$


再前进一步，将它修改为：


$$
\begin{bmatrix}

\begin{array}{ccc|c}
1 &0 &0 &a \\
0 &1 &0 &b \\
0 &0 &1 &c \\
\end{array}
    
\end{bmatrix}
$$


我们就可以直接得到：


$$
\begin{align*}
x = a \\
y = b \\
z = c
\end{align*}
$$



这便是高斯-约旦消元法，具体步骤可参考下图：

<img src="https://enpei-md.oss-cn-hangzhou.aliyuncs.com/img20210715213550.png?x-oss-process=style/wp" style="zoom:50%;" />





